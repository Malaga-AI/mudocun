{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudocun: feasibility study\n",
    "\n",
    "In this notebook we run a few sample queries to explore the concept of creating questions involving understanding of images, charts or formulas.\n",
    "\n",
    "First, let's setup the system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.DEBUG,\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    stream=sys.stdout,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quiz from a locally available pdf article\n",
    "\n",
    "Let's generate a sample quiz based on the paper \"Towards Autotuning by Alternating Communication Methods\", locally available at `./docs/autotuning_cscs.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 18:37:39,128 - DEBUG - Generating quiz for document \"Towards Autotuning by Alternating Communication Methods\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atineose/software_engineer/Malaga-AI/repos/mudocun/.venv/lib/python3.12/site-packages/pydantic/_internal/_fields.py:160: UserWarning: Field \"model_response\" has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 18:37:41,416 - DEBUG - Making request: POST https://oauth2.googleapis.com/token\n",
      "2024-05-30 18:37:43,521 - DEBUG - Starting new HTTPS connection (1): oauth2.googleapis.com:443\n",
      "2024-05-30 18:37:45,351 - DEBUG - https://oauth2.googleapis.com:443 \"POST /token HTTP/1.1\" 200 None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Quiz Questions\\n\\n**Figure 1: Autotuning framework**\\n\\n**Question 1:** Which of the following statements accurately describes the \"code refactoring\" stage of the autotuning framework?\\n\\nA. It involves benchmarking different communication methods and creating a knowledge database.\\nB. It generates different kernels with varying combinations of communication methods.\\nC. It transforms the code to explicitly expose the communication pattern for one-sided primitives.\\nD. It evaluates the performance of generated kernels under different running conditions.\\n\\n**Answer: C**\\n\\n**Question 2:**  In the \"Platform profile\" stage, what is the purpose of running microbenchmarks?\\n\\nA. To refactor the code and expose the communication pattern.\\nB. To generate different kernels with varying communication methods.\\nC. To determine the optimal kernel version for specific running conditions.\\nD. To gather platform-specific performance data for different communication operations.\\n\\n**Answer: D**\\n\\n**Question 3:** What is the key advantage of allowing \"out-of-order message delivery\" in the refactored code?\\n\\nA. It simplifies the code and makes it easier to understand.\\nB. It reduces the amount of data that needs to be communicated.\\nC. It enables the hardware and runtime to optimize communication scheduling.\\nD. It eliminates the need for synchronization mechanisms like barriers and fences. \\n\\n**Answer: C**\\n\\n**Question 4:**  Which of the following is NOT a communication method tested within the autotuning framework?\\n\\nA. UPC (Unified Parallel C)\\nB. MPI (Message Passing Interface)\\nC. OpenMP (Open Multi-Processing)\\nD. DMAPP (Distributed Memory Access Programming Protocol)\\n\\n**Answer: C** \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_questions\n",
    "from docs import local_documents\n",
    "from models import Article\n",
    "\n",
    "questions, creation_metadata = generate_questions(Article(**local_documents[0]))\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'publishers/google/models/gemini-1.5-pro-001',\n",
       " 'region': 'europe-west9',\n",
       " 'num_input_tokens': 538,\n",
       " 'num_output_tokens': 359,\n",
       " 'generation_time': 17.34649682044983,\n",
       " 'timestamp': '2024-05-30 16:37:56.477483+00:00'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creation_metadata.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Quiz Questions\n",
       "\n",
       "**Figure 1: Autotuning framework**\n",
       "\n",
       "**Question 1:** Which of the following statements accurately describes the \"code refactoring\" stage of the autotuning framework?\n",
       "\n",
       "A. It involves benchmarking different communication methods and creating a knowledge database.\n",
       "B. It generates different kernels with varying combinations of communication methods.\n",
       "C. It transforms the code to explicitly expose the communication pattern for one-sided primitives.\n",
       "D. It evaluates the performance of generated kernels under different running conditions.\n",
       "\n",
       "**Answer: C**\n",
       "\n",
       "**Question 2:**  In the \"Platform profile\" stage, what is the purpose of running microbenchmarks?\n",
       "\n",
       "A. To refactor the code and expose the communication pattern.\n",
       "B. To generate different kernels with varying communication methods.\n",
       "C. To determine the optimal kernel version for specific running conditions.\n",
       "D. To gather platform-specific performance data for different communication operations.\n",
       "\n",
       "**Answer: D**\n",
       "\n",
       "**Question 3:** What is the key advantage of allowing \"out-of-order message delivery\" in the refactored code?\n",
       "\n",
       "A. It simplifies the code and makes it easier to understand.\n",
       "B. It reduces the amount of data that needs to be communicated.\n",
       "C. It enables the hardware and runtime to optimize communication scheduling.\n",
       "D. It eliminates the need for synchronization mechanisms like barriers and fences. \n",
       "\n",
       "**Answer: C**\n",
       "\n",
       "**Question 4:**  Which of the following is NOT a communication method tested within the autotuning framework?\n",
       "\n",
       "A. UPC (Unified Parallel C)\n",
       "B. MPI (Message Passing Interface)\n",
       "C. OpenMP (Open Multi-Processing)\n",
       "D. DMAPP (Distributed Memory Access Programming Protocol)\n",
       "\n",
       "**Answer: C** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the model produces a quiz based on the figure included in the paper.\n",
    "\n",
    "There is currently a limitation that prevents from getting reproducible model responses (see [here](https://www.googlecloudcommunity.com/gc/AI-ML/Unexpected-Behavior-Gemini-1-0-Pro-002-Returns-Different-Outputs/m-p/757351/highlight/true#M7495)).\n",
    "\n",
    "Next, we analyze in detail one of the generations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a quiz generation\n",
    "\n",
    "The following quiz was generated in an ealier request (though it can't be reproduced exactly again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on the Scientific Article:\n",
       "\n",
       "**Figure 1: Autotuning framework**\n",
       "\n",
       "**Question 1:** What is the purpose of code refactoring in this autotuning framework?\n",
       "\n",
       "A. To benchmark different communication methods on the target platform.\n",
       "B. To generate multiple kernel versions with varying communication strategies.\n",
       "C. To expose the communication pattern and enable the use of one-sided primitives.\n",
       "D. To profile the performance of various communication methods for different message sizes.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\n",
       "\n",
       "A. Code refactoring\n",
       "B. Platform profiling\n",
       "C. Performance prediction\n",
       "D. Code generation\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\n",
       "\n",
       "A. Information about the specific scientific problem being solved.\n",
       "B. Knowledge about the underlying hardware architecture of the platform.\n",
       "C. Understanding of the communication patterns within the application code.\n",
       "D. Expertise in optimizing communication using different programming models.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\n",
       "\n",
       "A. Reduced code complexity compared to message-passing approaches.\n",
       "B. Improved portability of the code across different HPC platforms.\n",
       "C. Increased flexibility for the hardware and runtime to optimize communication.\n",
       "D. Elimination of the need for synchronization constructs like barriers and fences.\n",
       "\n",
       "**Correct Answer: C**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cscs_paper_questions = '## Multiple Choice Quiz Questions based on the Scientific Article:\\n\\n**Figure 1: Autotuning framework**\\n\\n**Question 1:** What is the purpose of code refactoring in this autotuning framework?\\n\\nA. To benchmark different communication methods on the target platform.\\nB. To generate multiple kernel versions with varying communication strategies.\\nC. To expose the communication pattern and enable the use of one-sided primitives.\\nD. To profile the performance of various communication methods for different message sizes.\\n\\n**Correct Answer: C**\\n\\n**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\\n\\nA. Code refactoring\\nB. Platform profiling\\nC. Performance prediction\\nD. Code generation\\n\\n**Correct Answer: C**\\n\\n**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\\n\\nA. Information about the specific scientific problem being solved.\\nB. Knowledge about the underlying hardware architecture of the platform.\\nC. Understanding of the communication patterns within the application code.\\nD. Expertise in optimizing communication using different programming models.\\n\\n**Correct Answer: C**\\n\\n**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\\n\\nA. Reduced code complexity compared to message-passing approaches.\\nB. Improved portability of the code across different HPC platforms.\\nC. Increased flexibility for the hardware and runtime to optimize communication.\\nD. Elimination of the need for synchronization constructs like barriers and fences.\\n\\n**Correct Answer: C**\\n'\n",
    "display(Markdown(cscs_paper_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty impressive. We can highlight the following aspects:\n",
    "* All generated questions correctly match the content of the figure.\n",
    "* There are multiple and diverse questions, involving several parts of the figure.\n",
    "* The multiple choice options are relevant, and use scientific vocabulary consistent with the paper's content.\n",
    "* All answers are correct, except Q3, where the correct answer would be A. Arguably, this was not sufficiently explained in the paper, so the model assumed a different kind of domain knowledge. (Interestingly, this is hinting at a weakness in the figure)\n",
    "* We observe a sophisticated behavior in Q4, where the correct answer considers information provided in the surrounding text, not in the figure itself. In particular, the correct answer is derived from the following extract in page 1:\n",
    "> Our autotuning framework is composed of three basic stages as shown in Fig. 1. First is code refactoring, which exposes the communication pattern so that it can be expressed with one-sided communication primitives. When possible, out-of-order message delivery is tolerated too. This transformation allows for maximum flexibility for the hardware and runtime to schedule the communication in the most efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quiz from an internet-hosted article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now a quiz generation from a larger paper, \"Mixtral of Experts\" by Mistral.ai, fetched on demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 18:37:56,522 - DEBUG - Generating quiz for document \"Mixtral of Experts\"...\n",
      "2024-05-30 18:37:56,525 - DEBUG - Starting new HTTPS connection (1): arxiv.org:443\n",
      "2024-05-30 18:37:57,621 - DEBUG - https://arxiv.org:443 \"GET /pdf/2401.04088 HTTP/1.1\" 200 2475990\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"## Figure 1: Mixture of Experts Layer\\n\\n**Question:** How does the Mixture of Experts Layer determine its output?\\n\\nA) It averages the outputs from all expert blocks.\\nB) It selects the output from the expert block with the highest confidence. \\nC) It calculates a weighted sum of outputs from two expert blocks chosen by a router.\\nD) It passes the input through all expert blocks sequentially and combines the results.\\n\\n**Correct Answer:** C) It calculates a weighted sum of outputs from two expert blocks chosen by a router.\\n\\n\\n## Table 1: Model Architecture\\n\\n**Question:**  What is the context length used in the Mixtral model architecture?\\n\\nA) 128 tokens\\nB) 32k tokens\\nC) 4096 tokens\\nD) 32000 tokens\\n\\n**Correct Answer:** B) 32k tokens\\n\\n\\n## Figure 2: Performance Comparison on Benchmarks\\n\\n**Question:** What do the results presented in Figure 2 generally indicate about Mixtral's performance compared to Llama 2 models?\\n\\nA) Mixtral consistently underperforms Llama 2 across all benchmark categories.\\nB) Mixtral shows comparable or superior performance to Llama 2 70B despite using fewer active parameters.\\nC) Mixtral's performance is only better than the smallest Llama 2 models, not the larger ones.\\nD) There is no discernible pattern in the performance comparison between Mixtral and Llama 2.\\n\\n**Correct Answer:** B) Mixtral shows comparable or superior performance to Llama 2 70B despite using fewer active parameters.\\n\\n\\n## Figure 4: Long Range Performance of Mixtral\\n\\n**Question:** What conclusion can be drawn from the right graph in Figure 4, regarding Mixtral's perplexity on the proof-pile dataset?\\n\\nA) Perplexity remains constant regardless of context length, suggesting limited long-range understanding. \\nB) Perplexity fluctuates unpredictably with increasing context length, indicating instability in handling long sequences.\\nC) Perplexity decreases with longer contexts, demonstrating Mixtral's ability to leverage information from extended sequences.\\nD) Perplexity increases with longer contexts, highlighting the model's difficulty in processing and understanding large amounts of text.\\n\\n**Correct Answer:** C) Perplexity decreases with longer contexts, demonstrating Mixtral's ability to leverage information from extended sequences. \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docs import online_documents\n",
    "\n",
    "questions, creation_metadata = generate_questions(Article(**online_documents[0]))\n",
    "questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'publishers/google/models/gemini-1.5-pro-001',\n",
       " 'region': 'europe-west9',\n",
       " 'num_input_tokens': 3376,\n",
       " 'num_output_tokens': 503,\n",
       " 'generation_time': 30.29361319541931,\n",
       " 'timestamp': '2024-05-30 16:38:46.675494+00:00'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "creation_metadata.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Figure 1: Mixture of Experts Layer\n",
       "\n",
       "**Question:** How does the Mixture of Experts Layer determine its output?\n",
       "\n",
       "A) It averages the outputs from all expert blocks.\n",
       "B) It selects the output from the expert block with the highest confidence. \n",
       "C) It calculates a weighted sum of outputs from two expert blocks chosen by a router.\n",
       "D) It passes the input through all expert blocks sequentially and combines the results.\n",
       "\n",
       "**Correct Answer:** C) It calculates a weighted sum of outputs from two expert blocks chosen by a router.\n",
       "\n",
       "\n",
       "## Table 1: Model Architecture\n",
       "\n",
       "**Question:**  What is the context length used in the Mixtral model architecture?\n",
       "\n",
       "A) 128 tokens\n",
       "B) 32k tokens\n",
       "C) 4096 tokens\n",
       "D) 32000 tokens\n",
       "\n",
       "**Correct Answer:** B) 32k tokens\n",
       "\n",
       "\n",
       "## Figure 2: Performance Comparison on Benchmarks\n",
       "\n",
       "**Question:** What do the results presented in Figure 2 generally indicate about Mixtral's performance compared to Llama 2 models?\n",
       "\n",
       "A) Mixtral consistently underperforms Llama 2 across all benchmark categories.\n",
       "B) Mixtral shows comparable or superior performance to Llama 2 70B despite using fewer active parameters.\n",
       "C) Mixtral's performance is only better than the smallest Llama 2 models, not the larger ones.\n",
       "D) There is no discernible pattern in the performance comparison between Mixtral and Llama 2.\n",
       "\n",
       "**Correct Answer:** B) Mixtral shows comparable or superior performance to Llama 2 70B despite using fewer active parameters.\n",
       "\n",
       "\n",
       "## Figure 4: Long Range Performance of Mixtral\n",
       "\n",
       "**Question:** What conclusion can be drawn from the right graph in Figure 4, regarding Mixtral's perplexity on the proof-pile dataset?\n",
       "\n",
       "A) Perplexity remains constant regardless of context length, suggesting limited long-range understanding. \n",
       "B) Perplexity fluctuates unpredictably with increasing context length, indicating instability in handling long sequences.\n",
       "C) Perplexity decreases with longer contexts, demonstrating Mixtral's ability to leverage information from extended sequences.\n",
       "D) Perplexity increases with longer contexts, highlighting the model's difficulty in processing and understanding large amounts of text.\n",
       "\n",
       "**Correct Answer:** C) Perplexity decreases with longer contexts, demonstrating Mixtral's ability to leverage information from extended sequences. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a larger quiz generation\n",
    "\n",
    "Similarly to the previous example, we analyze next a previous generation in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\n",
       "\n",
       "**Figure 1: Mixture of Experts Layer**\n",
       "\n",
       "**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\n",
       "\n",
       "A) 1\n",
       "B) 2  **[CORRECT]**\n",
       "C) 4\n",
       "D) 8\n",
       "\n",
       "**Figure 2: Performance of Mixtral and different Llama models**\n",
       "\n",
       "**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\n",
       "\n",
       "A) Comprehension\n",
       "B) Knowledge\n",
       "C) Code  **[CORRECT]**\n",
       "D) AGI Eval\n",
       "\n",
       "**Table 2: Comparison of Mixtral with Llama**\n",
       "\n",
       "**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\n",
       "\n",
       "A) 7B\n",
       "B) 13B  **[CORRECT]**\n",
       "C) 33B\n",
       "D) 70B\n",
       "\n",
       "**Figure 3: Results on MMLU, commonsense reasoning, etc.**\n",
       "\n",
       "**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\n",
       "\n",
       "A) Math\n",
       "B) Code\n",
       "C) Reasoning\n",
       "D) Comprehension  **[CORRECT]**\n",
       "\n",
       "**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\n",
       "\n",
       "**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\n",
       "\n",
       "A) 10.9%  **[CORRECT]**\n",
       "B) 8.4%\n",
       "C) 4.9%\n",
       "D) 2.3%\n",
       "\n",
       "**Figure 4: Long range performance of Mixtral**\n",
       "\n",
       "**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral's ability on the Passkey task?\n",
       "\n",
       "A) Mixtral's perplexity decreases with longer context.\n",
       "B) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\n",
       "C) Mixtral struggles with retrieving passkeys in long sequences.\n",
       "D) Mixtral's performance is comparable to other models on this task.\n",
       "\n",
       "**Figure 5: Bias Benchmarks**\n",
       "\n",
       "**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral's performance on bias benchmarks?\n",
       "\n",
       "A) Mixtral exhibits higher bias.\n",
       "B) Mixtral exhibits similar bias.\n",
       "C) Mixtral exhibits lower bias.  **[CORRECT]**\n",
       "D) The figure does not provide information about bias.\n",
       "\n",
       "**Figure 6: LMSys Leaderboard**\n",
       "\n",
       "**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \n",
       "\n",
       "A) First  **[CORRECT]**\n",
       "B) Second\n",
       "C) Third\n",
       "D) Fourth\n",
       "\n",
       "**Figure 7: Proportion of tokens assigned to each expert**\n",
       "\n",
       "**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\n",
       "\n",
       "A) Yes, experts clearly specialize in specific domains.\n",
       "B) No, there is no clear pattern of specialization.  **[CORRECT]**\n",
       "C) The figure does not provide information about expert specialization.\n",
       "D) Only the first layer shows a clear pattern of specialization.\n",
       "\n",
       "**Table 5: Percentage of expert assignment repetitions**\n",
       "\n",
       "**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\n",
       "\n",
       "A) Repetitions are significantly lower at higher layers.\n",
       "B) Repetitions are similar across all layers.\n",
       "C) Repetitions are significantly higher at higher layers.  **[CORRECT]**\n",
       "D) The table does not provide information about expert assignment repetitions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mixtral_questions = '## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\\n\\n**Figure 1: Mixture of Experts Layer**\\n\\n**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\\n\\nA) 1\\nB) 2  **[CORRECT]**\\nC) 4\\nD) 8\\n\\n**Figure 2: Performance of Mixtral and different Llama models**\\n\\n**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\\n\\nA) Comprehension\\nB) Knowledge\\nC) Code  **[CORRECT]**\\nD) AGI Eval\\n\\n**Table 2: Comparison of Mixtral with Llama**\\n\\n**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\\n\\nA) 7B\\nB) 13B  **[CORRECT]**\\nC) 33B\\nD) 70B\\n\\n**Figure 3: Results on MMLU, commonsense reasoning, etc.**\\n\\n**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\\n\\nA) Math\\nB) Code\\nC) Reasoning\\nD) Comprehension  **[CORRECT]**\\n\\n**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\\n\\n**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\\n\\nA) 10.9%  **[CORRECT]**\\nB) 8.4%\\nC) 4.9%\\nD) 2.3%\\n\\n**Figure 4: Long range performance of Mixtral**\\n\\n**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral\\'s ability on the Passkey task?\\n\\nA) Mixtral\\'s perplexity decreases with longer context.\\nB) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\\nC) Mixtral struggles with retrieving passkeys in long sequences.\\nD) Mixtral\\'s performance is comparable to other models on this task.\\n\\n**Figure 5: Bias Benchmarks**\\n\\n**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\'s performance on bias benchmarks?\\n\\nA) Mixtral exhibits higher bias.\\nB) Mixtral exhibits similar bias.\\nC) Mixtral exhibits lower bias.  **[CORRECT]**\\nD) The figure does not provide information about bias.\\n\\n**Figure 6: LMSys Leaderboard**\\n\\n**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \\n\\nA) First  **[CORRECT]**\\nB) Second\\nC) Third\\nD) Fourth\\n\\n**Figure 7: Proportion of tokens assigned to each expert**\\n\\n**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\\n\\nA) Yes, experts clearly specialize in specific domains.\\nB) No, there is no clear pattern of specialization.  **[CORRECT]**\\nC) The figure does not provide information about expert specialization.\\nD) Only the first layer shows a clear pattern of specialization.\\n\\n**Table 5: Percentage of expert assignment repetitions**\\n\\n**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\\n\\nA) Repetitions are significantly lower at higher layers.\\nB) Repetitions are similar across all layers.\\nC) Repetitions are significantly higher at higher layers.  **[CORRECT]**\\nD) The table does not provide information about expert assignment repetitions. \\n'\n",
    "display(Markdown(mixtral_questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are remarkably good. We observe the following:\n",
    "* Every question is correctly referring to one figure or table, even in the case where the authors inconsistently name a table as Figure 5.\n",
    "* All multiple choice options are consistent with the content of the paper.\n",
    "* All answers marked as correct are accurate.\n",
    "* The quiz exhibits comprehension of a broad range of diagrams and figures. It also clearly takes into account at least the text accompanying each figure or table (caption).\n",
    "* In one case, for question 5, the model even performs a substraction, adding a layer of reasoning beyond simply extracting/reading information.\n",
    "\n",
    "We observe the following limitations:\n",
    "* The format is different from the one chosen previously, so it'd be hard to post-process to extract question options and correct answer, among other aspects of interest.\n",
    "* The model stops at 10 questions, even though there are more figures and tables in the document. Other generations generate even fewer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured generation\n",
    "\n",
    "In order to be able to create a dataset by automated means, it'd be desirable to have a consistent output format that we can easily parse. In other words, we need \"structured generation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-05-30 18:38:46,737 - DEBUG - Generating structured quiz...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'text': 'In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?',\n",
       "  'correct_answer_idx': 1.0,\n",
       "  'choices': ['1', '2', '4', '8']},\n",
       " {'text': 'According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?',\n",
       "  'correct_answer_idx': 2.0,\n",
       "  'choices': ['Comprehension', 'Knowledge', 'Code', 'AGI Eval']},\n",
       " {'choices': ['7B', '13B', '33B', '70B'],\n",
       "  'correct_answer_idx': 1.0,\n",
       "  'text': 'According to Table 2, how many active parameters does Mixtral 8x7B use during inference?'},\n",
       " {'choices': ['Math', 'Code', 'Reasoning', 'Comprehension'],\n",
       "  'correct_answer_idx': 3.0,\n",
       "  'text': 'Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?'},\n",
       " {'choices': ['10.9%', '8.4%', '4.9%', '2.3%'],\n",
       "  'correct_answer_idx': 0.0,\n",
       "  'text': 'On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?'},\n",
       " {'text': \"What does the left part of Figure 4 demonstrate about Mixtral\\\\'s ability on the Passkey task?\",\n",
       "  'correct_answer_idx': 1.0,\n",
       "  'choices': [\"Mixtral\\\\'s perplexity decreases with longer context.\",\n",
       "   'Mixtral achieves perfect retrieval accuracy regardless of context length.',\n",
       "   'Mixtral struggles with retrieving passkeys in long sequences.',\n",
       "   \"Mixtral\\\\'s performance is comparable to other models on this task.\"]},\n",
       " {'choices': ['Mixtral exhibits higher bias.',\n",
       "   'Mixtral exhibits similar bias.',\n",
       "   'Mixtral exhibits lower bias.',\n",
       "   'The figure does not provide information about bias.'],\n",
       "  'correct_answer_idx': 2.0,\n",
       "  'text': \"Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\\\'s performance on bias benchmarks?\"},\n",
       " {'choices': ['First', 'Second', 'Third', 'Fourth'],\n",
       "  'correct_answer_idx': 0.0,\n",
       "  'text': 'As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models?'},\n",
       " {'choices': ['Yes, experts clearly specialize in specific domains.',\n",
       "   'No, there is no clear pattern of specialization.',\n",
       "   'The figure does not provide information about expert specialization.',\n",
       "   'Only the first layer shows a clear pattern of specialization.'],\n",
       "  'correct_answer_idx': 1.0,\n",
       "  'text': 'According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?'},\n",
       " {'choices': ['Repetitions are significantly lower at higher layers.',\n",
       "   'Repetitions are similar across all layers.',\n",
       "   'Repetitions are significantly higher at higher layers.',\n",
       "   'The table does not provide information about expert assignment repetitions.'],\n",
       "  'correct_answer_idx': 2.0,\n",
       "  'text': 'What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_structured_questions\n",
    "\n",
    "structured_questions, structuring_metadata = generate_structured_questions(mixtral_questions)\n",
    "structured_questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe how we could convert the generated questions into a structured format, which is much more useful for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'publishers/google/models/gemini-1.5-pro-001',\n",
       " 'region': 'europe-west9',\n",
       " 'num_input_tokens': 982,\n",
       " 'num_output_tokens': 541,\n",
       " 'generation_time': 16.000974893569946,\n",
       " 'timestamp': '2024-05-30 16:39:02.739906+00:00'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structuring_metadata.model_dump()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a full Quiz object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models import QuizMetadata, QuizQuestion, QuestionMetadata, Quiz\n",
    "\n",
    "quiz_metadata = QuizMetadata(creation_metadata=creation_metadata, structuring_metadata=structuring_metadata)\n",
    "quiz_questions = [\n",
    "    QuizQuestion(multiple_choice_question=q, metadata=QuestionMetadata(is_validated=False))\n",
    "    for q in structured_questions\n",
    "]\n",
    "quiz = Quiz(article=Article(**online_documents[0]), questions=quiz_questions, metadata=quiz_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': {'title': 'Mixtral of Experts',\n",
       "  'uri': 'https://arxiv.org/pdf/2401.04088'},\n",
       " 'questions': [{'multiple_choice_question': {'text': 'In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?',\n",
       "    'choices': ['1', '2', '4', '8'],\n",
       "    'correct_answer_idx': 1},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?',\n",
       "    'choices': ['Comprehension', 'Knowledge', 'Code', 'AGI Eval'],\n",
       "    'correct_answer_idx': 2},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'According to Table 2, how many active parameters does Mixtral 8x7B use during inference?',\n",
       "    'choices': ['7B', '13B', '33B', '70B'],\n",
       "    'correct_answer_idx': 1},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?',\n",
       "    'choices': ['Math', 'Code', 'Reasoning', 'Comprehension'],\n",
       "    'correct_answer_idx': 3},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?',\n",
       "    'choices': ['10.9%', '8.4%', '4.9%', '2.3%'],\n",
       "    'correct_answer_idx': 0},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': \"What does the left part of Figure 4 demonstrate about Mixtral\\\\'s ability on the Passkey task?\",\n",
       "    'choices': [\"Mixtral\\\\'s perplexity decreases with longer context.\",\n",
       "     'Mixtral achieves perfect retrieval accuracy regardless of context length.',\n",
       "     'Mixtral struggles with retrieving passkeys in long sequences.',\n",
       "     \"Mixtral\\\\'s performance is comparable to other models on this task.\"],\n",
       "    'correct_answer_idx': 1},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': \"Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\\\'s performance on bias benchmarks?\",\n",
       "    'choices': ['Mixtral exhibits higher bias.',\n",
       "     'Mixtral exhibits similar bias.',\n",
       "     'Mixtral exhibits lower bias.',\n",
       "     'The figure does not provide information about bias.'],\n",
       "    'correct_answer_idx': 2},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models?',\n",
       "    'choices': ['First', 'Second', 'Third', 'Fourth'],\n",
       "    'correct_answer_idx': 0},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?',\n",
       "    'choices': ['Yes, experts clearly specialize in specific domains.',\n",
       "     'No, there is no clear pattern of specialization.',\n",
       "     'The figure does not provide information about expert specialization.',\n",
       "     'Only the first layer shows a clear pattern of specialization.'],\n",
       "    'correct_answer_idx': 1},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}},\n",
       "  {'multiple_choice_question': {'text': 'What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?',\n",
       "    'choices': ['Repetitions are significantly lower at higher layers.',\n",
       "     'Repetitions are similar across all layers.',\n",
       "     'Repetitions are significantly higher at higher layers.',\n",
       "     'The table does not provide information about expert assignment repetitions.'],\n",
       "    'correct_answer_idx': 2},\n",
       "   'metadata': {'is_validated': False,\n",
       "    'validator': None,\n",
       "    'explanation': None}}],\n",
       " 'metadata': {'creation_metadata': {'model': 'publishers/google/models/gemini-1.5-pro-001',\n",
       "   'region': 'europe-west9',\n",
       "   'num_input_tokens': 3376,\n",
       "   'num_output_tokens': 503,\n",
       "   'generation_time': 30.29361319541931,\n",
       "   'timestamp': '2024-05-30 16:38:46.675494+00:00'},\n",
       "  'structuring_metadata': {'model': 'publishers/google/models/gemini-1.5-pro-001',\n",
       "   'region': 'europe-west9',\n",
       "   'num_input_tokens': 982,\n",
       "   'num_output_tokens': 541,\n",
       "   'generation_time': 16.000974893569946,\n",
       "   'timestamp': '2024-05-30 16:39:02.739906+00:00'}}}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz.model_dump()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
