{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudocun: prompt exploration\n",
    "\n",
    "In this notebook we run a few sample queries to explore the concept of creating questions involving understanding of images, charts or formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Towards Autotuning by Alternating Communication Methods\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Multiple Choice Quiz Questions based on the Scientific Article:\\n\\n**Figure 1:**\\n\\n**Question 1:** What does the \"Code Refactoring\" stage in Figure 1 primarily aim to achieve?\\n\\nA. Optimize the code for specific hardware platforms.\\nB. Identify the most efficient communication method.\\nC. **Expose and generalize the communication pattern.**\\nD. Generate different kernel versions for evaluation.\\n\\n**Question 2:** In the \"Platform Profile\" stage, what is the purpose of the microbenchmark?\\n\\nA. To determine the optimal problem size for the kernel.\\nB. **To measure the performance of basic communication operations.**\\nC. To generate code for different communication methods.\\nD. To evaluate the efficiency of the refactored code.\\n\\n**Question 3:**  According to Figure 1, what is one key difference between \"Kernel i\" and \"Kernel i+1\" in the \"Code Generation\" stage?\\n\\nA. Kernel i uses blocking communication while Kernel i+1 uses non-blocking communication.\\nB. **Kernel i uses MPI for communication while Kernel i+1 uses UPC.**\\nC. Kernel i processes remote data in order while Kernel i+1 processes it out of order.\\nD. Kernel i is optimized for Cray XE6 while Kernel i+1 is optimized for SGI Altix UV 1000.\\n\\n**Formula/Equation (Not explicitly mentioned):**\\n\\nSince the article doesn\\'t provide a specific formula, let\\'s create a question based on the concept of performance improvement:\\n\\n**Question 4:** The article mentions achieving performance improvements of up to 35% and 80% for bandwidth- and latency-bound kernels, respectively. What is a common metric used to quantify such performance improvements?\\n\\nA. **Speedup**\\nB. Latency\\nC. Bandwidth\\nD. Throughput \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_quiz\n",
    "from docs import local_documents\n",
    "\n",
    "response, input_tokens, output_tokens, gen_time = generate_quiz(local_documents[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 389, 11.351142883300781)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens, gen_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on the Scientific Article:\n",
       "\n",
       "**Figure 1:**\n",
       "\n",
       "**Question 1:** What does the \"Code Refactoring\" stage in Figure 1 primarily aim to achieve?\n",
       "\n",
       "A. Optimize the code for specific hardware platforms.\n",
       "B. Identify the most efficient communication method.\n",
       "C. **Expose and generalize the communication pattern.**\n",
       "D. Generate different kernel versions for evaluation.\n",
       "\n",
       "**Question 2:** In the \"Platform Profile\" stage, what is the purpose of the microbenchmark?\n",
       "\n",
       "A. To determine the optimal problem size for the kernel.\n",
       "B. **To measure the performance of basic communication operations.**\n",
       "C. To generate code for different communication methods.\n",
       "D. To evaluate the efficiency of the refactored code.\n",
       "\n",
       "**Question 3:**  According to Figure 1, what is one key difference between \"Kernel i\" and \"Kernel i+1\" in the \"Code Generation\" stage?\n",
       "\n",
       "A. Kernel i uses blocking communication while Kernel i+1 uses non-blocking communication.\n",
       "B. **Kernel i uses MPI for communication while Kernel i+1 uses UPC.**\n",
       "C. Kernel i processes remote data in order while Kernel i+1 processes it out of order.\n",
       "D. Kernel i is optimized for Cray XE6 while Kernel i+1 is optimized for SGI Altix UV 1000.\n",
       "\n",
       "**Formula/Equation (Not explicitly mentioned):**\n",
       "\n",
       "Since the article doesn't provide a specific formula, let's create a question based on the concept of performance improvement:\n",
       "\n",
       "**Question 4:** The article mentions achieving performance improvements of up to 35% and 80% for bandwidth- and latency-bound kernels, respectively. What is a common metric used to quantify such performance improvements?\n",
       "\n",
       "A. **Speedup**\n",
       "B. Latency\n",
       "C. Bandwidth\n",
       "D. Throughput \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pending: evaluate results\n",
    "\n",
    "Let's try now a quiz generation from a larger paper, fetched on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Mixtral of Experts\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"## Figure 1: Mixture of Experts Layer Quiz\\n\\n**Question:** How does the Mixture of Experts Layer depicted in Figure 1 function?\\n\\n**A)** The router processes the input and distributes it equally to all 8 experts.\\n**B)** Each expert processes the input independently, and the outputs are averaged.\\n**C)** The router selects two experts based on the input, and their weighted outputs are combined.\\n**D)** The experts communicate with each other to determine the best output.\\n\\n**Correct Answer:** **C)** The router selects two experts based on the input, and their weighted outputs are combined.\\n\\n\\n## Table 1: Model Architecture Quiz\\n\\n**Question:** According to Table 1, what is the context length used in the Mixtral model architecture?\\n\\n**A)** 128 tokens\\n**B)** 32,768 tokens\\n**C)** 4,096 tokens\\n**D)** 32,000 tokens\\n\\n**Correct Answer:** **B)** 32,768 tokens\\n\\n\\n## Figure 2: Performance Comparison Quiz\\n\\n**Question:** What does Figure 2 illustrate about Mixtral's performance compared to different Llama models?\\n\\n**A)** Mixtral consistently underperforms compared to all Llama models.\\n**B)** Mixtral shows significant improvement in reading comprehension tasks.\\n**C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\\n**D)** Mixtral's performance is identical to Llama 2 70B across all benchmarks.\\n\\n**Correct Answer:** **C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\\n\\n\\n## Figure 4: Long Range Performance Quiz\\n\\n**Question:** What conclusion can be drawn from the right panel of Figure 4 regarding Mixtral's perplexity on the proof-pile dataset?\\n\\n**A)** Perplexity remains constant regardless of context length.\\n**B)** Perplexity increases as the context length increases.\\n**C)** Perplexity fluctuates unpredictably with increasing context length.\\n**D)** Perplexity decreases as the context length increases.\\n\\n**Correct Answer:** **D)** Perplexity decreases as the context length increases. \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docs import online_documents\n",
    "\n",
    "response, input_tokens, output_tokens, gen_time = generate_quiz(online_documents[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3376, 484, 13.825375080108643)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens, gen_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Figure 1: Mixture of Experts Layer Quiz\n",
       "\n",
       "**Question:** How does the Mixture of Experts Layer depicted in Figure 1 function?\n",
       "\n",
       "**A)** The router processes the input and distributes it equally to all 8 experts.\n",
       "**B)** Each expert processes the input independently, and the outputs are averaged.\n",
       "**C)** The router selects two experts based on the input, and their weighted outputs are combined.\n",
       "**D)** The experts communicate with each other to determine the best output.\n",
       "\n",
       "**Correct Answer:** **C)** The router selects two experts based on the input, and their weighted outputs are combined.\n",
       "\n",
       "\n",
       "## Table 1: Model Architecture Quiz\n",
       "\n",
       "**Question:** According to Table 1, what is the context length used in the Mixtral model architecture?\n",
       "\n",
       "**A)** 128 tokens\n",
       "**B)** 32,768 tokens\n",
       "**C)** 4,096 tokens\n",
       "**D)** 32,000 tokens\n",
       "\n",
       "**Correct Answer:** **B)** 32,768 tokens\n",
       "\n",
       "\n",
       "## Figure 2: Performance Comparison Quiz\n",
       "\n",
       "**Question:** What does Figure 2 illustrate about Mixtral's performance compared to different Llama models?\n",
       "\n",
       "**A)** Mixtral consistently underperforms compared to all Llama models.\n",
       "**B)** Mixtral shows significant improvement in reading comprehension tasks.\n",
       "**C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\n",
       "**D)** Mixtral's performance is identical to Llama 2 70B across all benchmarks.\n",
       "\n",
       "**Correct Answer:** **C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\n",
       "\n",
       "\n",
       "## Figure 4: Long Range Performance Quiz\n",
       "\n",
       "**Question:** What conclusion can be drawn from the right panel of Figure 4 regarding Mixtral's perplexity on the proof-pile dataset?\n",
       "\n",
       "**A)** Perplexity remains constant regardless of context length.\n",
       "**B)** Perplexity increases as the context length increases.\n",
       "**C)** Perplexity fluctuates unpredictably with increasing context length.\n",
       "**D)** Perplexity decreases as the context length increases.\n",
       "\n",
       "**Correct Answer:** **D)** Perplexity decreases as the context length increases. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pending: evaluate results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
