{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudocun: prompt exploration\n",
    "\n",
    "In this notebook we run a few sample queries to explore the concept of creating questions involving understanding of images, charts or formulas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Towards Autotuning by Alternating Communication Methods\"...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atineose/software_engineer/Malaga-AI/repos/mudocun/.venv/lib/python3.12/site-packages/google/auth/_default.py:76: UserWarning: Your application has authenticated using end user credentials from Google Cloud SDK without a quota project. You might receive a \"quota exceeded\" or \"API not enabled\" error. See the following page for troubleshooting: https://cloud.google.com/docs/authentication/adc-troubleshooting/user-creds. \n",
      "  warnings.warn(_CLOUD_SDK_CREDENTIALS_WARNING)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Multiple Choice Quiz Questions based on the Scientific Article:\\n\\n**Figure 1: Autotuning framework**\\n\\n**Question 1:** What is the purpose of code refactoring in this autotuning framework?\\n\\nA. To benchmark different communication methods on the target platform.\\nB. To generate multiple kernel versions with varying communication strategies.\\nC. To expose the communication pattern and enable the use of one-sided primitives.\\nD. To profile the performance of various communication methods for different message sizes.\\n\\n**Correct Answer: C**\\n\\n**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\\n\\nA. Code refactoring\\nB. Platform profiling\\nC. Performance prediction\\nD. Code generation\\n\\n**Correct Answer: C**\\n\\n**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\\n\\nA. Information about the specific scientific problem being solved.\\nB. Knowledge about the underlying hardware architecture of the platform.\\nC. Understanding of the communication patterns within the application code.\\nD. Expertise in optimizing communication using different programming models.\\n\\n**Correct Answer: C**\\n\\n**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\\n\\nA. Reduced code complexity compared to message-passing approaches.\\nB. Improved portability of the code across different HPC platforms.\\nC. Increased flexibility for the hardware and runtime to optimize communication.\\nD. Elimination of the need for synchronization constructs like barriers and fences.\\n\\n**Correct Answer: C**\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_quiz\n",
    "from docs import local_documents\n",
    "\n",
    "response = generate_quiz(local_documents[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on the Scientific Article:\n",
       "\n",
       "**Figure 1: Autotuning framework**\n",
       "\n",
       "**Question 1:** What is the purpose of code refactoring in this autotuning framework?\n",
       "\n",
       "A. To benchmark different communication methods on the target platform.\n",
       "B. To generate multiple kernel versions with varying communication strategies.\n",
       "C. To expose the communication pattern and enable the use of one-sided primitives.\n",
       "D. To profile the performance of various communication methods for different message sizes.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\n",
       "\n",
       "A. Code refactoring\n",
       "B. Platform profiling\n",
       "C. Performance prediction\n",
       "D. Code generation\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\n",
       "\n",
       "A. Information about the specific scientific problem being solved.\n",
       "B. Knowledge about the underlying hardware architecture of the platform.\n",
       "C. Understanding of the communication patterns within the application code.\n",
       "D. Expertise in optimizing communication using different programming models.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\n",
       "\n",
       "A. Reduced code complexity compared to message-passing approaches.\n",
       "B. Improved portability of the code across different HPC platforms.\n",
       "C. Increased flexibility for the hardware and runtime to optimize communication.\n",
       "D. Elimination of the need for synchronization constructs like barriers and fences.\n",
       "\n",
       "**Correct Answer: C**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty impressive. We can highlight the following aspects:\n",
    "* All generated questions correctly match the content of the figure\n",
    "* There are multiple and diverse questions, involving several parts of the figure\n",
    "* The multiple choice options are relevant, and use scientific vocabulary consistent with the paper's content.\n",
    "* All answers are correct, except Q3, where the correct answer would be A. Arguably, this was not sufficiently explained in the paper, so the model assumed a different kind of domain knowledge. (Interestingly, this is hinting at a weakness in the figure)\n",
    "\n",
    "Let's try now a quiz generation from a larger paper, fetched on demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Mixtral of Experts\"...\n",
      "candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      text: \"## Multiple Choice Quiz Questions based on \\\"Mixtral of Experts\\\"\\n\\n**Figure 1: Mixture of Experts Layer**\\n\\n**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\\n\\nA) 1\\nB) 2  **[CORRECT]**\\nC) 4\\nD) 8\\n\\n**Figure 2: Performance of Mixtral and different Llama models**\\n\\n**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\\n\\nA) Comprehension\\nB) Knowledge\\nC) Code  **[CORRECT]**\\nD) AGI Eval\\n\\n**Table 2: Comparison of Mixtral with Llama**\\n\\n**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\\n\\nA) 7B\\nB) 13B  **[CORRECT]**\\nC) 33B\\nD) 70B\\n\\n**Figure 3: Results on MMLU, commonsense reasoning, etc.**\\n\\n**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\\n\\nA) Math\\nB) Code\\nC) Reasoning\\nD) Comprehension  **[CORRECT]**\\n\\n**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\\n\\n**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\\n\\nA) 10.9%  **[CORRECT]**\\nB) 8.4%\\nC) 4.9%\\nD) 2.3%\\n\\n**Figure 4: Long range performance of Mixtral**\\n\\n**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral\\'s ability on the Passkey task?\\n\\nA) Mixtral\\'s perplexity decreases with longer context.\\nB) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\\nC) Mixtral struggles with retrieving passkeys in long sequences.\\nD) Mixtral\\'s performance is comparable to other models on this task.\\n\\n**Figure 5: Bias Benchmarks**\\n\\n**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\'s performance on bias benchmarks?\\n\\nA) Mixtral exhibits higher bias.\\nB) Mixtral exhibits similar bias.\\nC) Mixtral exhibits lower bias.  **[CORRECT]**\\nD) The figure does not provide information about bias.\\n\\n**Figure 6: LMSys Leaderboard**\\n\\n**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \\n\\nA) First  **[CORRECT]**\\nB) Second\\nC) Third\\nD) Fourth\\n\\n**Figure 7: Proportion of tokens assigned to each expert**\\n\\n**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\\n\\nA) Yes, experts clearly specialize in specific domains.\\nB) No, there is no clear pattern of specialization.  **[CORRECT]**\\nC) The figure does not provide information about expert specialization.\\nD) Only the first layer shows a clear pattern of specialization.\\n\\n**Table 5: Percentage of expert assignment repetitions**\\n\\n**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\\n\\nA) Repetitions are significantly lower at higher layers.\\nB) Repetitions are similar across all layers.\\nC) Repetitions are significantly higher at higher layers.  **[CORRECT]**\\nD) The table does not provide information about expert assignment repetitions. \\n\"\n",
      "    }\n",
      "  }\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.18227993\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0875638425\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0870968252\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0834411755\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.245627463\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.130839705\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0818127096\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0867867395\n",
      "  }\n",
      "}\n",
      "usage_metadata {\n",
      "  prompt_token_count: 3376\n",
      "  candidates_token_count: 873\n",
      "  total_token_count: 4249\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\\n\\n**Figure 1: Mixture of Experts Layer**\\n\\n**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\\n\\nA) 1\\nB) 2  **[CORRECT]**\\nC) 4\\nD) 8\\n\\n**Figure 2: Performance of Mixtral and different Llama models**\\n\\n**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\\n\\nA) Comprehension\\nB) Knowledge\\nC) Code  **[CORRECT]**\\nD) AGI Eval\\n\\n**Table 2: Comparison of Mixtral with Llama**\\n\\n**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\\n\\nA) 7B\\nB) 13B  **[CORRECT]**\\nC) 33B\\nD) 70B\\n\\n**Figure 3: Results on MMLU, commonsense reasoning, etc.**\\n\\n**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\\n\\nA) Math\\nB) Code\\nC) Reasoning\\nD) Comprehension  **[CORRECT]**\\n\\n**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\\n\\n**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\\n\\nA) 10.9%  **[CORRECT]**\\nB) 8.4%\\nC) 4.9%\\nD) 2.3%\\n\\n**Figure 4: Long range performance of Mixtral**\\n\\n**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral\\'s ability on the Passkey task?\\n\\nA) Mixtral\\'s perplexity decreases with longer context.\\nB) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\\nC) Mixtral struggles with retrieving passkeys in long sequences.\\nD) Mixtral\\'s performance is comparable to other models on this task.\\n\\n**Figure 5: Bias Benchmarks**\\n\\n**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\'s performance on bias benchmarks?\\n\\nA) Mixtral exhibits higher bias.\\nB) Mixtral exhibits similar bias.\\nC) Mixtral exhibits lower bias.  **[CORRECT]**\\nD) The figure does not provide information about bias.\\n\\n**Figure 6: LMSys Leaderboard**\\n\\n**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \\n\\nA) First  **[CORRECT]**\\nB) Second\\nC) Third\\nD) Fourth\\n\\n**Figure 7: Proportion of tokens assigned to each expert**\\n\\n**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\\n\\nA) Yes, experts clearly specialize in specific domains.\\nB) No, there is no clear pattern of specialization.  **[CORRECT]**\\nC) The figure does not provide information about expert specialization.\\nD) Only the first layer shows a clear pattern of specialization.\\n\\n**Table 5: Percentage of expert assignment repetitions**\\n\\n**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\\n\\nA) Repetitions are significantly lower at higher layers.\\nB) Repetitions are similar across all layers.\\nC) Repetitions are significantly higher at higher layers.  **[CORRECT]**\\nD) The table does not provide information about expert assignment repetitions. \\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_quiz\n",
    "from docs import online_documents\n",
    "\n",
    "response = generate_quiz(online_documents[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\n",
       "\n",
       "**Figure 1: Mixture of Experts Layer**\n",
       "\n",
       "**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\n",
       "\n",
       "A) 1\n",
       "B) 2  **[CORRECT]**\n",
       "C) 4\n",
       "D) 8\n",
       "\n",
       "**Figure 2: Performance of Mixtral and different Llama models**\n",
       "\n",
       "**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\n",
       "\n",
       "A) Comprehension\n",
       "B) Knowledge\n",
       "C) Code  **[CORRECT]**\n",
       "D) AGI Eval\n",
       "\n",
       "**Table 2: Comparison of Mixtral with Llama**\n",
       "\n",
       "**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\n",
       "\n",
       "A) 7B\n",
       "B) 13B  **[CORRECT]**\n",
       "C) 33B\n",
       "D) 70B\n",
       "\n",
       "**Figure 3: Results on MMLU, commonsense reasoning, etc.**\n",
       "\n",
       "**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\n",
       "\n",
       "A) Math\n",
       "B) Code\n",
       "C) Reasoning\n",
       "D) Comprehension  **[CORRECT]**\n",
       "\n",
       "**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\n",
       "\n",
       "**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\n",
       "\n",
       "A) 10.9%  **[CORRECT]**\n",
       "B) 8.4%\n",
       "C) 4.9%\n",
       "D) 2.3%\n",
       "\n",
       "**Figure 4: Long range performance of Mixtral**\n",
       "\n",
       "**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral's ability on the Passkey task?\n",
       "\n",
       "A) Mixtral's perplexity decreases with longer context.\n",
       "B) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\n",
       "C) Mixtral struggles with retrieving passkeys in long sequences.\n",
       "D) Mixtral's performance is comparable to other models on this task.\n",
       "\n",
       "**Figure 5: Bias Benchmarks**\n",
       "\n",
       "**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral's performance on bias benchmarks?\n",
       "\n",
       "A) Mixtral exhibits higher bias.\n",
       "B) Mixtral exhibits similar bias.\n",
       "C) Mixtral exhibits lower bias.  **[CORRECT]**\n",
       "D) The figure does not provide information about bias.\n",
       "\n",
       "**Figure 6: LMSys Leaderboard**\n",
       "\n",
       "**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \n",
       "\n",
       "A) First  **[CORRECT]**\n",
       "B) Second\n",
       "C) Third\n",
       "D) Fourth\n",
       "\n",
       "**Figure 7: Proportion of tokens assigned to each expert**\n",
       "\n",
       "**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\n",
       "\n",
       "A) Yes, experts clearly specialize in specific domains.\n",
       "B) No, there is no clear pattern of specialization.  **[CORRECT]**\n",
       "C) The figure does not provide information about expert specialization.\n",
       "D) Only the first layer shows a clear pattern of specialization.\n",
       "\n",
       "**Table 5: Percentage of expert assignment repetitions**\n",
       "\n",
       "**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\n",
       "\n",
       "A) Repetitions are significantly lower at higher layers.\n",
       "B) Repetitions are similar across all layers.\n",
       "C) Repetitions are significantly higher at higher layers.  **[CORRECT]**\n",
       "D) The table does not provide information about expert assignment repetitions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are remarkably good. We observe the following:\n",
    "* Every question is correctly referring to one figure or table, even in the case where the authors inconsistently name a table as Figure 5.\n",
    "* All multiple choice options are consistent with the content of the paper.\n",
    "* All answers marked as correct are accurate.\n",
    "* The quiz exhibits comprehension of a broad range of diagrams and figures. It also clearly takes into account the text accompanying each figure or table.\n",
    "* In one case, for question 5, the model even performs a substraction, adding a layer of reasoning beyond simply extracting/reading information.\n",
    "\n",
    "We observe the following limitations:\n",
    "* The format is different from the one chosen previously, so it'd be hard to post-process to extract question options and correct answer, among other aspects of interest.\n",
    "* The model stops at 10 questions, even though there are more figures and tables in the document. It has probably been conditioned to prefer such number when generating answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
