{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudocun: prompt exploration\n",
    "\n",
    "In this notebook we run a few sample queries to explore the concept of creating questions involving understanding of images, charts or formulas.\n",
    "\n",
    "First, let's setup the system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quiz from a locally available pdf article\n",
    "\n",
    "Let's generate a sample quiz based on the paper \"Towards Autotuning by Alternating Communication Methods\", locally available at `./docs/autotuning_cscs.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Towards Autotuning by Alternating Communication Methods\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Multiple Choice Quiz Questions\\n\\n**Figure 1:**\\n\\n**Question 1:** What is the purpose of the \"Code Refactoring\" stage in the autotuning framework?\\n\\nA. To benchmark different communication methods on the target platform.\\nB. To generate multiple kernel versions with varying communication strategies.\\nC. **To restructure the code to expose and generalize the communication pattern.**\\nD. To profile the performance of the refactored code with different parameters.\\n\\n**Question 2:** In the \"Platform Profile\" stage, what kind of information is gathered?\\n\\nA. The number of processors and memory available on the system.\\nB. **Performance characteristics of different communication methods for various message sizes and processor counts.**\\nC. The optimal communication strategy for the specific kernel being tuned.\\nD. The source code of the kernel and its dependencies.\\n\\n**Question 3:** What is the role of the \"Sync\" operation in the refactored kernel code?\\n\\nA. To send data to remote processors.\\nB. To receive data from remote processors.\\nC. **To ensure that data dependencies are met before proceeding with computation.**\\nD. To measure the time taken for communication.\\n\\n**Question 4:** What is the main advantage of using one-sided communication primitives like \"upc_memput\" over traditional MPI send/receive operations?\\n\\nA. They are easier to implement and debug.\\nB. They always result in lower communication overhead.\\nC. **They allow for greater overlap of communication and computation.**\\nD. They are supported by all HPC platforms.\\n\\n\\n**Please note:** This quiz only covers Figure 1. You can create similar questions for other figures or formulas in the article. \\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_quiz\n",
    "from docs import local_documents\n",
    "\n",
    "response, input_tokens, output_tokens, gen_time = generate_quiz(local_documents[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 354, 9.38943886756897)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens, gen_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions\n",
       "\n",
       "**Figure 1:**\n",
       "\n",
       "**Question 1:** What is the purpose of the \"Code Refactoring\" stage in the autotuning framework?\n",
       "\n",
       "A. To benchmark different communication methods on the target platform.\n",
       "B. To generate multiple kernel versions with varying communication strategies.\n",
       "C. **To restructure the code to expose and generalize the communication pattern.**\n",
       "D. To profile the performance of the refactored code with different parameters.\n",
       "\n",
       "**Question 2:** In the \"Platform Profile\" stage, what kind of information is gathered?\n",
       "\n",
       "A. The number of processors and memory available on the system.\n",
       "B. **Performance characteristics of different communication methods for various message sizes and processor counts.**\n",
       "C. The optimal communication strategy for the specific kernel being tuned.\n",
       "D. The source code of the kernel and its dependencies.\n",
       "\n",
       "**Question 3:** What is the role of the \"Sync\" operation in the refactored kernel code?\n",
       "\n",
       "A. To send data to remote processors.\n",
       "B. To receive data from remote processors.\n",
       "C. **To ensure that data dependencies are met before proceeding with computation.**\n",
       "D. To measure the time taken for communication.\n",
       "\n",
       "**Question 4:** What is the main advantage of using one-sided communication primitives like \"upc_memput\" over traditional MPI send/receive operations?\n",
       "\n",
       "A. They are easier to implement and debug.\n",
       "B. They always result in lower communication overhead.\n",
       "C. **They allow for greater overlap of communication and computation.**\n",
       "D. They are supported by all HPC platforms.\n",
       "\n",
       "\n",
       "**Please note:** This quiz only covers Figure 1. You can create similar questions for other figures or formulas in the article. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the model produces a quiz based on the figure included in the paper.\n",
    "\n",
    "There is currently a limitation that prevents from getting reproducible model responses (see [here](https://www.googlecloudcommunity.com/gc/AI-ML/Unexpected-Behavior-Gemini-1-0-Pro-002-Returns-Different-Outputs/m-p/757351/highlight/true#M7495)).\n",
    "\n",
    "Next, we analyze in detail one of the generations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a quiz generation\n",
    "\n",
    "The following quiz was generated in an ealier request (though it can't be reproduced exactly again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on the Scientific Article:\n",
       "\n",
       "**Figure 1: Autotuning framework**\n",
       "\n",
       "**Question 1:** What is the purpose of code refactoring in this autotuning framework?\n",
       "\n",
       "A. To benchmark different communication methods on the target platform.\n",
       "B. To generate multiple kernel versions with varying communication strategies.\n",
       "C. To expose the communication pattern and enable the use of one-sided primitives.\n",
       "D. To profile the performance of various communication methods for different message sizes.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\n",
       "\n",
       "A. Code refactoring\n",
       "B. Platform profiling\n",
       "C. Performance prediction\n",
       "D. Code generation\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\n",
       "\n",
       "A. Information about the specific scientific problem being solved.\n",
       "B. Knowledge about the underlying hardware architecture of the platform.\n",
       "C. Understanding of the communication patterns within the application code.\n",
       "D. Expertise in optimizing communication using different programming models.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\n",
       "\n",
       "A. Reduced code complexity compared to message-passing approaches.\n",
       "B. Improved portability of the code across different HPC platforms.\n",
       "C. Increased flexibility for the hardware and runtime to optimize communication.\n",
       "D. Elimination of the need for synchronization constructs like barriers and fences.\n",
       "\n",
       "**Correct Answer: C**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cscs_paper_response = '## Multiple Choice Quiz Questions based on the Scientific Article:\\n\\n**Figure 1: Autotuning framework**\\n\\n**Question 1:** What is the purpose of code refactoring in this autotuning framework?\\n\\nA. To benchmark different communication methods on the target platform.\\nB. To generate multiple kernel versions with varying communication strategies.\\nC. To expose the communication pattern and enable the use of one-sided primitives.\\nD. To profile the performance of various communication methods for different message sizes.\\n\\n**Correct Answer: C**\\n\\n**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\\n\\nA. Code refactoring\\nB. Platform profiling\\nC. Performance prediction\\nD. Code generation\\n\\n**Correct Answer: C**\\n\\n**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\\n\\nA. Information about the specific scientific problem being solved.\\nB. Knowledge about the underlying hardware architecture of the platform.\\nC. Understanding of the communication patterns within the application code.\\nD. Expertise in optimizing communication using different programming models.\\n\\n**Correct Answer: C**\\n\\n**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\\n\\nA. Reduced code complexity compared to message-passing approaches.\\nB. Improved portability of the code across different HPC platforms.\\nC. Increased flexibility for the hardware and runtime to optimize communication.\\nD. Elimination of the need for synchronization constructs like barriers and fences.\\n\\n**Correct Answer: C**\\n'\n",
    "display(Markdown(cscs_paper_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty impressive. We can highlight the following aspects:\n",
    "* All generated questions correctly match the content of the figure.\n",
    "* There are multiple and diverse questions, involving several parts of the figure.\n",
    "* The multiple choice options are relevant, and use scientific vocabulary consistent with the paper's content.\n",
    "* All answers are correct, except Q3, where the correct answer would be A. Arguably, this was not sufficiently explained in the paper, so the model assumed a different kind of domain knowledge. (Interestingly, this is hinting at a weakness in the figure)\n",
    "* We observe a sophisticated behavior in Q4, where the correct answer considers information provided in the surrounding text, not in the figure itself. In particular, the correct answer is derived from the following extract in page 1:\n",
    "> Our autotuning framework is composed of three basic stages as shown in Fig. 1. First is code refactoring, which exposes the communication pattern so that it can be expressed with one-sided communication primitives. When possible, out-of-order message delivery is tolerated too. This transformation allows for maximum flexibility for the hardware and runtime to schedule the communication in the most efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quiz from an internet-hosted article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now a quiz generation from a larger paper, \"Mixtral of Experts\" by Mistral.ai, fetched on demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Mixtral of Experts\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"## Figure 1: Mixture of Experts Layer Quiz\\n\\n**Question:** How does the Mixture of Experts Layer depicted in Figure 1 function?\\n\\n**A)** The router processes the input and distributes it equally to all 8 experts.\\n**B)** Each expert processes the input independently, and the outputs are averaged.\\n**C)** The router selects two experts based on the input, and their weighted outputs are combined.\\n**D)** The experts communicate with each other to determine the best output.\\n\\n**Correct Answer:** **C)** The router selects two experts based on the input, and their weighted outputs are combined.\\n\\n\\n## Table 1: Model Architecture Quiz\\n\\n**Question:** According to Table 1, what is the context length used in the Mixtral model architecture?\\n\\n**A)** 128 tokens\\n**B)** 32,768 tokens\\n**C)** 4,096 tokens\\n**D)** 32,000 tokens\\n\\n**Correct Answer:** **B)** 32,768 tokens\\n\\n\\n## Figure 2: Performance Comparison Quiz\\n\\n**Question:** What does Figure 2 illustrate about Mixtral's performance compared to different Llama models?\\n\\n**A)** Mixtral consistently underperforms compared to all Llama models.\\n**B)** Mixtral shows significant improvement in reading comprehension tasks.\\n**C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\\n**D)** Mixtral's performance is identical to Llama 2 70B across all benchmarks.\\n\\n**Correct Answer:** **C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\\n\\n\\n## Figure 4: Long Range Performance Quiz\\n\\n**Question:** What conclusion can be drawn from the right panel of Figure 4 regarding Mixtral's perplexity on the proof-pile dataset?\\n\\n**A)** Perplexity remains constant regardless of context length.\\n**B)** Perplexity increases as the context length increases.\\n**C)** Perplexity fluctuates unpredictably with increasing context length.\\n**D)** Perplexity decreases as the context length increases.\\n\\n**Correct Answer:** **D)** Perplexity decreases as the context length increases. \\n\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docs import online_documents\n",
    "\n",
    "response, input_tokens, output_tokens, gen_time = generate_quiz(online_documents[0])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3376, 484, 13.825375080108643)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens, gen_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Figure 1: Mixture of Experts Layer Quiz\n",
       "\n",
       "**Question:** How does the Mixture of Experts Layer depicted in Figure 1 function?\n",
       "\n",
       "**A)** The router processes the input and distributes it equally to all 8 experts.\n",
       "**B)** Each expert processes the input independently, and the outputs are averaged.\n",
       "**C)** The router selects two experts based on the input, and their weighted outputs are combined.\n",
       "**D)** The experts communicate with each other to determine the best output.\n",
       "\n",
       "**Correct Answer:** **C)** The router selects two experts based on the input, and their weighted outputs are combined.\n",
       "\n",
       "\n",
       "## Table 1: Model Architecture Quiz\n",
       "\n",
       "**Question:** According to Table 1, what is the context length used in the Mixtral model architecture?\n",
       "\n",
       "**A)** 128 tokens\n",
       "**B)** 32,768 tokens\n",
       "**C)** 4,096 tokens\n",
       "**D)** 32,000 tokens\n",
       "\n",
       "**Correct Answer:** **B)** 32,768 tokens\n",
       "\n",
       "\n",
       "## Figure 2: Performance Comparison Quiz\n",
       "\n",
       "**Question:** What does Figure 2 illustrate about Mixtral's performance compared to different Llama models?\n",
       "\n",
       "**A)** Mixtral consistently underperforms compared to all Llama models.\n",
       "**B)** Mixtral shows significant improvement in reading comprehension tasks.\n",
       "**C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\n",
       "**D)** Mixtral's performance is identical to Llama 2 70B across all benchmarks.\n",
       "\n",
       "**Correct Answer:** **C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\n",
       "\n",
       "\n",
       "## Figure 4: Long Range Performance Quiz\n",
       "\n",
       "**Question:** What conclusion can be drawn from the right panel of Figure 4 regarding Mixtral's perplexity on the proof-pile dataset?\n",
       "\n",
       "**A)** Perplexity remains constant regardless of context length.\n",
       "**B)** Perplexity increases as the context length increases.\n",
       "**C)** Perplexity fluctuates unpredictably with increasing context length.\n",
       "**D)** Perplexity decreases as the context length increases.\n",
       "\n",
       "**Correct Answer:** **D)** Perplexity decreases as the context length increases. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a larger quiz generation\n",
    "\n",
    "Similarly to the previous example, we analyze next a previous generation in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\n",
       "\n",
       "**Figure 1: Mixture of Experts Layer**\n",
       "\n",
       "**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\n",
       "\n",
       "A) 1\n",
       "B) 2  **[CORRECT]**\n",
       "C) 4\n",
       "D) 8\n",
       "\n",
       "**Figure 2: Performance of Mixtral and different Llama models**\n",
       "\n",
       "**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\n",
       "\n",
       "A) Comprehension\n",
       "B) Knowledge\n",
       "C) Code  **[CORRECT]**\n",
       "D) AGI Eval\n",
       "\n",
       "**Table 2: Comparison of Mixtral with Llama**\n",
       "\n",
       "**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\n",
       "\n",
       "A) 7B\n",
       "B) 13B  **[CORRECT]**\n",
       "C) 33B\n",
       "D) 70B\n",
       "\n",
       "**Figure 3: Results on MMLU, commonsense reasoning, etc.**\n",
       "\n",
       "**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\n",
       "\n",
       "A) Math\n",
       "B) Code\n",
       "C) Reasoning\n",
       "D) Comprehension  **[CORRECT]**\n",
       "\n",
       "**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\n",
       "\n",
       "**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\n",
       "\n",
       "A) 10.9%  **[CORRECT]**\n",
       "B) 8.4%\n",
       "C) 4.9%\n",
       "D) 2.3%\n",
       "\n",
       "**Figure 4: Long range performance of Mixtral**\n",
       "\n",
       "**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral's ability on the Passkey task?\n",
       "\n",
       "A) Mixtral's perplexity decreases with longer context.\n",
       "B) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\n",
       "C) Mixtral struggles with retrieving passkeys in long sequences.\n",
       "D) Mixtral's performance is comparable to other models on this task.\n",
       "\n",
       "**Figure 5: Bias Benchmarks**\n",
       "\n",
       "**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral's performance on bias benchmarks?\n",
       "\n",
       "A) Mixtral exhibits higher bias.\n",
       "B) Mixtral exhibits similar bias.\n",
       "C) Mixtral exhibits lower bias.  **[CORRECT]**\n",
       "D) The figure does not provide information about bias.\n",
       "\n",
       "**Figure 6: LMSys Leaderboard**\n",
       "\n",
       "**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \n",
       "\n",
       "A) First  **[CORRECT]**\n",
       "B) Second\n",
       "C) Third\n",
       "D) Fourth\n",
       "\n",
       "**Figure 7: Proportion of tokens assigned to each expert**\n",
       "\n",
       "**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\n",
       "\n",
       "A) Yes, experts clearly specialize in specific domains.\n",
       "B) No, there is no clear pattern of specialization.  **[CORRECT]**\n",
       "C) The figure does not provide information about expert specialization.\n",
       "D) Only the first layer shows a clear pattern of specialization.\n",
       "\n",
       "**Table 5: Percentage of expert assignment repetitions**\n",
       "\n",
       "**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\n",
       "\n",
       "A) Repetitions are significantly lower at higher layers.\n",
       "B) Repetitions are similar across all layers.\n",
       "C) Repetitions are significantly higher at higher layers.  **[CORRECT]**\n",
       "D) The table does not provide information about expert assignment repetitions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mixtral_response = '## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\\n\\n**Figure 1: Mixture of Experts Layer**\\n\\n**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\\n\\nA) 1\\nB) 2  **[CORRECT]**\\nC) 4\\nD) 8\\n\\n**Figure 2: Performance of Mixtral and different Llama models**\\n\\n**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\\n\\nA) Comprehension\\nB) Knowledge\\nC) Code  **[CORRECT]**\\nD) AGI Eval\\n\\n**Table 2: Comparison of Mixtral with Llama**\\n\\n**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\\n\\nA) 7B\\nB) 13B  **[CORRECT]**\\nC) 33B\\nD) 70B\\n\\n**Figure 3: Results on MMLU, commonsense reasoning, etc.**\\n\\n**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\\n\\nA) Math\\nB) Code\\nC) Reasoning\\nD) Comprehension  **[CORRECT]**\\n\\n**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\\n\\n**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\\n\\nA) 10.9%  **[CORRECT]**\\nB) 8.4%\\nC) 4.9%\\nD) 2.3%\\n\\n**Figure 4: Long range performance of Mixtral**\\n\\n**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral\\'s ability on the Passkey task?\\n\\nA) Mixtral\\'s perplexity decreases with longer context.\\nB) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\\nC) Mixtral struggles with retrieving passkeys in long sequences.\\nD) Mixtral\\'s performance is comparable to other models on this task.\\n\\n**Figure 5: Bias Benchmarks**\\n\\n**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\'s performance on bias benchmarks?\\n\\nA) Mixtral exhibits higher bias.\\nB) Mixtral exhibits similar bias.\\nC) Mixtral exhibits lower bias.  **[CORRECT]**\\nD) The figure does not provide information about bias.\\n\\n**Figure 6: LMSys Leaderboard**\\n\\n**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \\n\\nA) First  **[CORRECT]**\\nB) Second\\nC) Third\\nD) Fourth\\n\\n**Figure 7: Proportion of tokens assigned to each expert**\\n\\n**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\\n\\nA) Yes, experts clearly specialize in specific domains.\\nB) No, there is no clear pattern of specialization.  **[CORRECT]**\\nC) The figure does not provide information about expert specialization.\\nD) Only the first layer shows a clear pattern of specialization.\\n\\n**Table 5: Percentage of expert assignment repetitions**\\n\\n**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\\n\\nA) Repetitions are significantly lower at higher layers.\\nB) Repetitions are similar across all layers.\\nC) Repetitions are significantly higher at higher layers.  **[CORRECT]**\\nD) The table does not provide information about expert assignment repetitions. \\n'\n",
    "display(Markdown(mixtral_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are remarkably good. We observe the following:\n",
    "* Every question is correctly referring to one figure or table, even in the case where the authors inconsistently name a table as Figure 5.\n",
    "* All multiple choice options are consistent with the content of the paper.\n",
    "* All answers marked as correct are accurate.\n",
    "* The quiz exhibits comprehension of a broad range of diagrams and figures. It also clearly takes into account at least the text accompanying each figure or table (caption).\n",
    "* In one case, for question 5, the model even performs a substraction, adding a layer of reasoning beyond simply extracting/reading information.\n",
    "\n",
    "We observe the following limitations:\n",
    "* The format is different from the one chosen previously, so it'd be hard to post-process to extract question options and correct answer, among other aspects of interest.\n",
    "* The model stops at 10 questions, even though there are more figures and tables in the document. Other generations generate even fewer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured generation\n",
    "\n",
    "In order to be able to create a dataset by automated means, it'd be desirable to have a consistent output format that we can easily parse. In other words, we need \"structured generation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/atineose/software_engineer/Malaga-AI/repos/mudocun/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "response:\n",
       "GenerateContentResponse(\n",
       "    done=True,\n",
       "    iterator=None,\n",
       "    result=glm.GenerateContentResponse({\n",
       "      \"candidates\": [\n",
       "        {\n",
       "          \"content\": {\n",
       "            \"parts\": [\n",
       "              {\n",
       "                \"function_call\": {\n",
       "                  \"name\": \"add_to_database\",\n",
       "                  \"args\": {\n",
       "                    \"people\": [\n",
       "                      {\n",
       "                        \"description\": \"A young girl who possesses a magic backpack\",\n",
       "                        \"start_place_name\": \"Willow Creek\",\n",
       "                        \"name\": \"Anya\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"Anya\\\\'s kind-hearted mother\",\n",
       "                        \"start_place_name\": \"Willow Creek\",\n",
       "                        \"name\": \"Elise\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"Anya\\\\'s wise-bearded father\",\n",
       "                        \"start_place_name\": \"Willow Creek\",\n",
       "                        \"name\": \"Edward\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A curious and adventurous boy who is Anya\\\\'s best friend\",\n",
       "                        \"start_place_name\": \"Willow Creek\",\n",
       "                        \"name\": \"Samuel\"\n",
       "                      }\n",
       "                    ],\n",
       "                    \"things\": [\n",
       "                      {\n",
       "                        \"description\": \"A shimmering emerald-green backpack that holds magical objects\",\n",
       "                        \"name\": \"Magic Backpack\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A magical sword that can defeat monsters\",\n",
       "                        \"name\": \"Shimmering Sword\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A book containing ancient and powerful spells\",\n",
       "                        \"name\": \"Book of Ancient Spells\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A compass that always points north\",\n",
       "                        \"name\": \"Tiny Compass\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A key that can open any lock\",\n",
       "                        \"name\": \"Magical Key\"\n",
       "                      }\n",
       "                    ],\n",
       "                    \"places\": [\n",
       "                      {\n",
       "                        \"description\": \"A quaint town nestled amidst rolling hills and whispering willows\",\n",
       "                        \"name\": \"Willow Creek\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A modest cottage in Willow Creek\",\n",
       "                        \"name\": \"Anya\\\\'s Cottage\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"The only schoolhouse in Willow Creek\",\n",
       "                        \"name\": \"Willow Creek Schoolhouse\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"description\": \"A shadowy forest surrounding Willow Creek\",\n",
       "                        \"name\": \"Forest\"\n",
       "                      }\n",
       "                    ],\n",
       "                    \"relationships\": [\n",
       "                      {\n",
       "                        \"person_1_name\": \"Anya\",\n",
       "                        \"person_2_name\": \"Elise\",\n",
       "                        \"relationship\": \"daughter\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"person_1_name\": \"Anya\",\n",
       "                        \"person_2_name\": \"Edward\",\n",
       "                        \"relationship\": \"daughter\"\n",
       "                      },\n",
       "                      {\n",
       "                        \"person_1_name\": \"Anya\",\n",
       "                        \"person_2_name\": \"Samuel\",\n",
       "                        \"relationship\": \"best friend\"\n",
       "                      }\n",
       "                    ]\n",
       "                  }\n",
       "                }\n",
       "              }\n",
       "            ],\n",
       "            \"role\": \"model\"\n",
       "          },\n",
       "          \"finish_reason\": 1,\n",
       "          \"index\": 0,\n",
       "          \"safety_ratings\": [\n",
       "            {\n",
       "              \"category\": 7,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 10,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 9,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            },\n",
       "            {\n",
       "              \"category\": 8,\n",
       "              \"probability\": 1,\n",
       "              \"blocked\": false\n",
       "            }\n",
       "          ],\n",
       "          \"token_count\": 0,\n",
       "          \"grounding_attributions\": []\n",
       "        }\n",
       "      ]\n",
       "    }),\n",
       ")"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path\n",
    "\n",
    "\n",
    "from tool_use import my_func\n",
    "my_func()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating structured quiz (publishers/google/models/gemini-1.5-pro-001)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path\n",
    "\n",
    "\n",
    "from vertexai_utils import generate_structured_quiz\n",
    "from docs import local_documents\n",
    "from models import Article\n",
    "\n",
    "questions = \"## Figure 1: Mixture of Experts Layer Quiz\\n\\n**Question:** How does the Mixture of Experts Layer depicted in Figure 1 function?\\n\\n**A)** The router processes the input and distributes it equally to all 8 experts.\\n**B)** Each expert processes the input independently, and the outputs are averaged.\\n**C)** The router selects two experts based on the input, and their weighted outputs are combined.\\n**D)** The experts communicate with each other to determine the best output.\\n\\n**Correct Answer:** **C)** The router selects two experts based on the input, and their weighted outputs are combined.\\n\\n\\n## Table 1: Model Architecture Quiz\\n\\n**Question:** According to Table 1, what is the context length used in the Mixtral model architecture?\\n\\n**A)** 128 tokens\\n**B)** 32,768 tokens\\n**C)** 4,096 tokens\\n**D)** 32,000 tokens\\n\\n**Correct Answer:** **B)** 32,768 tokens\\n\\n\\n## Figure 2: Performance Comparison Quiz\\n\\n**Question:** What does Figure 2 illustrate about Mixtral's performance compared to different Llama models?\\n\\n**A)** Mixtral consistently underperforms compared to all Llama models.\\n**B)** Mixtral shows significant improvement in reading comprehension tasks.\\n**C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\\n**D)** Mixtral's performance is identical to Llama 2 70B across all benchmarks.\\n\\n**Correct Answer:** **C)** Mixtral outperforms or matches Llama 2 70B on all benchmarks, particularly excelling in mathematics and code generation.\\n\\n\\n## Figure 4: Long Range Performance Quiz\\n\\n**Question:** What conclusion can be drawn from the right panel of Figure 4 regarding Mixtral's perplexity on the proof-pile dataset?\\n\\n**A)** Perplexity remains constant regardless of context length.\\n**B)** Perplexity increases as the context length increases.\\n**C)** Perplexity fluctuates unpredictably with increasing context length.\\n**D)** Perplexity decreases as the context length increases.\\n\\n**Correct Answer:** **D)** Perplexity decreases as the context length increases. \\n\"\n",
    "response = generate_structured_quiz(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "name: \"create_quiz\"\n",
       "args {\n",
       "  fields {\n",
       "    key: \"text\"\n",
       "    value {\n",
       "      string_value: \"How does the Mixture of Experts Layer depicted in Figure 1 function?\"\n",
       "    }\n",
       "  }\n",
       "  fields {\n",
       "    key: \"correct_answer_idx\"\n",
       "    value {\n",
       "      number_value: 2\n",
       "    }\n",
       "  }\n",
       "  fields {\n",
       "    key: \"choices\"\n",
       "    value {\n",
       "      list_value {\n",
       "        values {\n",
       "          string_value: \"The router processes the input and distributes it equally to all 8 experts.\"\n",
       "        }\n",
       "        values {\n",
       "          string_value: \"Each expert processes the input independently, and the outputs are averaged.\"\n",
       "        }\n",
       "        values {\n",
       "          string_value: \"The router selects two experts based on the input, and their weighted outputs are combined.\"\n",
       "        }\n",
       "        values {\n",
       "          string_value: \"The experts communicate with each other to determine the best output.\"\n",
       "        }\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func = response.candidates[0].content.parts[0].function_call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vertexai_utils import create_quiz\n",
    "\n",
    "r = func.name(**func.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Figure 1: Autotuning Framework Quiz\n",
       "\n",
       "**Question:** Which of the following stages in the autotuning framework involves creating a knowledge database of different communication methods for various processors and message sizes?\n",
       "\n",
       "A) Code refactoring\n",
       "B) Platform profiling\n",
       "C) Code generation\n",
       "D) Kernel execution\n",
       "\n",
       "**Correct Answer: B) Platform profiling**\n",
       "\n",
       "**Explanation:** Platform profiling involves understanding the performance characteristics of different communication methods on the target hardware. This includes creating a knowledge base of how these methods perform for different processors, message sizes, and other relevant factors. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.candidates[0].content.parts[0].text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
