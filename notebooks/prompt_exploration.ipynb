{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mudocun: prompt exploration\n",
    "\n",
    "In this notebook we run a few sample queries to explore the concept of creating questions involving understanding of images, charts or formulas.\n",
    "\n",
    "First, let's setup the system path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "import sys\n",
    "\n",
    "ROOT_DIR = str(pathlib.Path.cwd().parent)\n",
    "sys.path.append(ROOT_DIR) # Add parent folder to the path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quiz from a locally available pdf article\n",
    "\n",
    "Let's generate a sample quiz based on the paper \"Towards Autotuning by Alternating Communication Methods\", locally available at `./docs/autotuning_cscs.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Towards Autotuning by Alternating Communication Methods\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Multiple Choice Quiz Questions:\\n\\n**Figure 1: Autotuning framework**\\n\\n**Question 1:** What is the purpose of code refactoring in the autotuning framework?\\n\\nA. To optimize the code for a specific hardware architecture.\\nB. To expose the communication pattern and enable the use of one-sided communication primitives.\\nC. To generate multiple versions of the kernel with different communication methods.\\nD. To profile the communication performance of the kernel.\\n\\n**Correct Answer: B**\\n\\n**Question 2:** Which of the following is NOT a communication method tested in the platform profiling stage?\\n\\nA. UPC\\nB. MPI\\nC. OpenMP\\nD. DMAPP\\n\\n**Correct Answer: C**\\n\\n**Question 3:** What is the role of the platform specific knowledge base in the autotuning framework?\\n\\nA. To store the refactored code for different kernels.\\nB. To provide information about the performance of different communication methods on the target platform.\\nC. To generate the final optimized kernel code.\\nD. To execute and evaluate the performance of different kernel versions.\\n\\n**Correct Answer: B**\\n\\n**Question 4:**  What is the final output of the code generation stage?\\n\\nA. A single optimized kernel version.\\nB. Multiple kernel versions with different communication methods.\\nC. A detailed performance report of the original kernel.\\nD. A platform specific knowledge base.\\n\\n**Correct Answer: B** \\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from vertexai_utils import generate_quiz\n",
    "from docs import local_documents\n",
    "from models import Article\n",
    "\n",
    "response, input_tokens, output_tokens, gen_time = generate_quiz(Article(local_documents[0]))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(538, 303, 8.027336835861206)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens, gen_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions:\n",
       "\n",
       "**Figure 1: Autotuning framework**\n",
       "\n",
       "**Question 1:** What is the purpose of code refactoring in the autotuning framework?\n",
       "\n",
       "A. To optimize the code for a specific hardware architecture.\n",
       "B. To expose the communication pattern and enable the use of one-sided communication primitives.\n",
       "C. To generate multiple versions of the kernel with different communication methods.\n",
       "D. To profile the communication performance of the kernel.\n",
       "\n",
       "**Correct Answer: B**\n",
       "\n",
       "**Question 2:** Which of the following is NOT a communication method tested in the platform profiling stage?\n",
       "\n",
       "A. UPC\n",
       "B. MPI\n",
       "C. OpenMP\n",
       "D. DMAPP\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 3:** What is the role of the platform specific knowledge base in the autotuning framework?\n",
       "\n",
       "A. To store the refactored code for different kernels.\n",
       "B. To provide information about the performance of different communication methods on the target platform.\n",
       "C. To generate the final optimized kernel code.\n",
       "D. To execute and evaluate the performance of different kernel versions.\n",
       "\n",
       "**Correct Answer: B**\n",
       "\n",
       "**Question 4:**  What is the final output of the code generation stage?\n",
       "\n",
       "A. A single optimized kernel version.\n",
       "B. Multiple kernel versions with different communication methods.\n",
       "C. A detailed performance report of the original kernel.\n",
       "D. A platform specific knowledge base.\n",
       "\n",
       "**Correct Answer: B** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see the model produces a quiz based on the figure included in the paper.\n",
    "\n",
    "There is currently a limitation that prevents from getting reproducible model responses (see [here](https://www.googlecloudcommunity.com/gc/AI-ML/Unexpected-Behavior-Gemini-1-0-Pro-002-Returns-Different-Outputs/m-p/757351/highlight/true#M7495)).\n",
    "\n",
    "Next, we analyze in detail one of the generations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a quiz generation\n",
    "\n",
    "The following quiz was generated in an ealier request (though it can't be reproduced exactly again)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on the Scientific Article:\n",
       "\n",
       "**Figure 1: Autotuning framework**\n",
       "\n",
       "**Question 1:** What is the purpose of code refactoring in this autotuning framework?\n",
       "\n",
       "A. To benchmark different communication methods on the target platform.\n",
       "B. To generate multiple kernel versions with varying communication strategies.\n",
       "C. To expose the communication pattern and enable the use of one-sided primitives.\n",
       "D. To profile the performance of various communication methods for different message sizes.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\n",
       "\n",
       "A. Code refactoring\n",
       "B. Platform profiling\n",
       "C. Performance prediction\n",
       "D. Code generation\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\n",
       "\n",
       "A. Information about the specific scientific problem being solved.\n",
       "B. Knowledge about the underlying hardware architecture of the platform.\n",
       "C. Understanding of the communication patterns within the application code.\n",
       "D. Expertise in optimizing communication using different programming models.\n",
       "\n",
       "**Correct Answer: C**\n",
       "\n",
       "**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\n",
       "\n",
       "A. Reduced code complexity compared to message-passing approaches.\n",
       "B. Improved portability of the code across different HPC platforms.\n",
       "C. Increased flexibility for the hardware and runtime to optimize communication.\n",
       "D. Elimination of the need for synchronization constructs like barriers and fences.\n",
       "\n",
       "**Correct Answer: C**\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cscs_paper_response = '## Multiple Choice Quiz Questions based on the Scientific Article:\\n\\n**Figure 1: Autotuning framework**\\n\\n**Question 1:** What is the purpose of code refactoring in this autotuning framework?\\n\\nA. To benchmark different communication methods on the target platform.\\nB. To generate multiple kernel versions with varying communication strategies.\\nC. To expose the communication pattern and enable the use of one-sided primitives.\\nD. To profile the performance of various communication methods for different message sizes.\\n\\n**Correct Answer: C**\\n\\n**Question 2:** Which of the following is NOT a stage in the presented autotuning framework?\\n\\nA. Code refactoring\\nB. Platform profiling\\nC. Performance prediction\\nD. Code generation\\n\\n**Correct Answer: C**\\n\\n**Question 3:** What does the \"Domain knowledge\" arrow in Figure 1 represent?\\n\\nA. Information about the specific scientific problem being solved.\\nB. Knowledge about the underlying hardware architecture of the platform.\\nC. Understanding of the communication patterns within the application code.\\nD. Expertise in optimizing communication using different programming models.\\n\\n**Correct Answer: C**\\n\\n**Question 4:**  What is the main advantage of using one-sided communication primitives, as shown in the refactored kernel?\\n\\nA. Reduced code complexity compared to message-passing approaches.\\nB. Improved portability of the code across different HPC platforms.\\nC. Increased flexibility for the hardware and runtime to optimize communication.\\nD. Elimination of the need for synchronization constructs like barriers and fences.\\n\\n**Correct Answer: C**\\n'\n",
    "display(Markdown(cscs_paper_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results are pretty impressive. We can highlight the following aspects:\n",
    "* All generated questions correctly match the content of the figure.\n",
    "* There are multiple and diverse questions, involving several parts of the figure.\n",
    "* The multiple choice options are relevant, and use scientific vocabulary consistent with the paper's content.\n",
    "* All answers are correct, except Q3, where the correct answer would be A. Arguably, this was not sufficiently explained in the paper, so the model assumed a different kind of domain knowledge. (Interestingly, this is hinting at a weakness in the figure)\n",
    "* We observe a sophisticated behavior in Q4, where the correct answer considers information provided in the surrounding text, not in the figure itself. In particular, the correct answer is derived from the following extract in page 1:\n",
    "> Our autotuning framework is composed of three basic stages as shown in Fig. 1. First is code refactoring, which exposes the communication pattern so that it can be expressed with one-sided communication primitives. When possible, out-of-order message delivery is tolerated too. This transformation allows for maximum flexibility for the hardware and runtime to schedule the communication in the most efficient manner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate quiz from an internet-hosted article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try now a quiz generation from a larger paper, \"Mixtral of Experts\" by Mistral.ai, fetched on demand:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating quiz for document \"Mixtral of Experts\"...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'## Mixtral of Experts Quiz\\n\\n**Figure 1: Mixture of Experts Layer**\\n\\n**Question:**  What is the role of the \"router\" in the Mixture of Experts Layer?\\n\\n**(A)** It processes the input vector and generates the output vector.\\n**(B)** It contains a set of 8 expert networks, each with its own weights.\\n**(C)** It determines the weight assigned to the output of each expert.\\n**(D)** **It selects two out of the eight experts to process the input vector.**\\n\\n**Answer: (D)**\\n\\n**Table 1: Model architecture**\\n\\n**Question:** What is the context length used for training the Mixtral model?\\n\\n**(A)** 4096 tokens \\n**(B)** 14336 tokens\\n**(C)** **32768 tokens**\\n**(D)** 32000 tokens \\n\\n**Answer: (C)**\\n\\n**Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks.**\\n\\n**Question:** In which task category does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\\n\\n**(A)** Knowledge\\n**(B)** Comprehension\\n**(C)** **Math and Code**\\n**(D)** AGI Eval\\n\\n**Answer: (C)**\\n\\n**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5.**\\n\\n**Question:** On the MBPP benchmark, which model demonstrates the highest pass@1 accuracy? \\n\\n**(A)** Llama 2 70B\\n**(B)** GPT-3.5\\n**(C)** **Mixtral 8x7B**\\n**(D)** The information is not provided in the table.\\n\\n**Answer: (C)** \\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from docs import online_documents\n",
    "\n",
    "response, input_tokens, output_tokens, gen_time = generate_quiz(Article(online_documents[0]))\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3376, 375, 12.879631996154785)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_tokens, output_tokens, gen_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Mixtral of Experts Quiz\n",
       "\n",
       "**Figure 1: Mixture of Experts Layer**\n",
       "\n",
       "**Question:**  What is the role of the \"router\" in the Mixture of Experts Layer?\n",
       "\n",
       "**(A)** It processes the input vector and generates the output vector.\n",
       "**(B)** It contains a set of 8 expert networks, each with its own weights.\n",
       "**(C)** It determines the weight assigned to the output of each expert.\n",
       "**(D)** **It selects two out of the eight experts to process the input vector.**\n",
       "\n",
       "**Answer: (D)**\n",
       "\n",
       "**Table 1: Model architecture**\n",
       "\n",
       "**Question:** What is the context length used for training the Mixtral model?\n",
       "\n",
       "**(A)** 4096 tokens \n",
       "**(B)** 14336 tokens\n",
       "**(C)** **32768 tokens**\n",
       "**(D)** 32000 tokens \n",
       "\n",
       "**Answer: (C)**\n",
       "\n",
       "**Figure 2: Performance of Mixtral and different Llama models on a wide range of benchmarks.**\n",
       "\n",
       "**Question:** In which task category does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\n",
       "\n",
       "**(A)** Knowledge\n",
       "**(B)** Comprehension\n",
       "**(C)** **Math and Code**\n",
       "**(D)** AGI Eval\n",
       "\n",
       "**Answer: (C)**\n",
       "\n",
       "**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5.**\n",
       "\n",
       "**Question:** On the MBPP benchmark, which model demonstrates the highest pass@1 accuracy? \n",
       "\n",
       "**(A)** Llama 2 70B\n",
       "**(B)** GPT-3.5\n",
       "**(C)** **Mixtral 8x7B**\n",
       "**(D)** The information is not provided in the table.\n",
       "\n",
       "**Answer: (C)** \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of a larger quiz generation\n",
    "\n",
    "Similarly to the previous example, we analyze next a previous generation in detail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\n",
       "\n",
       "**Figure 1: Mixture of Experts Layer**\n",
       "\n",
       "**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\n",
       "\n",
       "A) 1\n",
       "B) 2  **[CORRECT]**\n",
       "C) 4\n",
       "D) 8\n",
       "\n",
       "**Figure 2: Performance of Mixtral and different Llama models**\n",
       "\n",
       "**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\n",
       "\n",
       "A) Comprehension\n",
       "B) Knowledge\n",
       "C) Code  **[CORRECT]**\n",
       "D) AGI Eval\n",
       "\n",
       "**Table 2: Comparison of Mixtral with Llama**\n",
       "\n",
       "**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\n",
       "\n",
       "A) 7B\n",
       "B) 13B  **[CORRECT]**\n",
       "C) 33B\n",
       "D) 70B\n",
       "\n",
       "**Figure 3: Results on MMLU, commonsense reasoning, etc.**\n",
       "\n",
       "**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\n",
       "\n",
       "A) Math\n",
       "B) Code\n",
       "C) Reasoning\n",
       "D) Comprehension  **[CORRECT]**\n",
       "\n",
       "**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\n",
       "\n",
       "**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\n",
       "\n",
       "A) 10.9%  **[CORRECT]**\n",
       "B) 8.4%\n",
       "C) 4.9%\n",
       "D) 2.3%\n",
       "\n",
       "**Figure 4: Long range performance of Mixtral**\n",
       "\n",
       "**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral's ability on the Passkey task?\n",
       "\n",
       "A) Mixtral's perplexity decreases with longer context.\n",
       "B) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\n",
       "C) Mixtral struggles with retrieving passkeys in long sequences.\n",
       "D) Mixtral's performance is comparable to other models on this task.\n",
       "\n",
       "**Figure 5: Bias Benchmarks**\n",
       "\n",
       "**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral's performance on bias benchmarks?\n",
       "\n",
       "A) Mixtral exhibits higher bias.\n",
       "B) Mixtral exhibits similar bias.\n",
       "C) Mixtral exhibits lower bias.  **[CORRECT]**\n",
       "D) The figure does not provide information about bias.\n",
       "\n",
       "**Figure 6: LMSys Leaderboard**\n",
       "\n",
       "**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \n",
       "\n",
       "A) First  **[CORRECT]**\n",
       "B) Second\n",
       "C) Third\n",
       "D) Fourth\n",
       "\n",
       "**Figure 7: Proportion of tokens assigned to each expert**\n",
       "\n",
       "**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\n",
       "\n",
       "A) Yes, experts clearly specialize in specific domains.\n",
       "B) No, there is no clear pattern of specialization.  **[CORRECT]**\n",
       "C) The figure does not provide information about expert specialization.\n",
       "D) Only the first layer shows a clear pattern of specialization.\n",
       "\n",
       "**Table 5: Percentage of expert assignment repetitions**\n",
       "\n",
       "**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\n",
       "\n",
       "A) Repetitions are significantly lower at higher layers.\n",
       "B) Repetitions are similar across all layers.\n",
       "C) Repetitions are significantly higher at higher layers.  **[CORRECT]**\n",
       "D) The table does not provide information about expert assignment repetitions. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "mixtral_response = '## Multiple Choice Quiz Questions based on \"Mixtral of Experts\"\\n\\n**Figure 1: Mixture of Experts Layer**\\n\\n**Question 1:** In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?\\n\\nA) 1\\nB) 2  **[CORRECT]**\\nC) 4\\nD) 8\\n\\n**Figure 2: Performance of Mixtral and different Llama models**\\n\\n**Question 2:**  According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?\\n\\nA) Comprehension\\nB) Knowledge\\nC) Code  **[CORRECT]**\\nD) AGI Eval\\n\\n**Table 2: Comparison of Mixtral with Llama**\\n\\n**Question 3:**  According to Table 2, how many active parameters does Mixtral 8x7B use during inference?\\n\\nA) 7B\\nB) 13B  **[CORRECT]**\\nC) 33B\\nD) 70B\\n\\n**Figure 3: Results on MMLU, commonsense reasoning, etc.**\\n\\n**Question 4:**  Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?\\n\\nA) Math\\nB) Code\\nC) Reasoning\\nD) Comprehension  **[CORRECT]**\\n\\n**Table 3: Comparison of Mixtral with Llama 2 70B and GPT-3.5**\\n\\n**Question 5:** On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?\\n\\nA) 10.9%  **[CORRECT]**\\nB) 8.4%\\nC) 4.9%\\nD) 2.3%\\n\\n**Figure 4: Long range performance of Mixtral**\\n\\n**Question 6:** What does the left part of Figure 4 demonstrate about Mixtral\\'s ability on the Passkey task?\\n\\nA) Mixtral\\'s perplexity decreases with longer context.\\nB) Mixtral achieves perfect retrieval accuracy regardless of context length.  **[CORRECT]**\\nC) Mixtral struggles with retrieving passkeys in long sequences.\\nD) Mixtral\\'s performance is comparable to other models on this task.\\n\\n**Figure 5: Bias Benchmarks**\\n\\n**Question 7:**  Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\'s performance on bias benchmarks?\\n\\nA) Mixtral exhibits higher bias.\\nB) Mixtral exhibits similar bias.\\nC) Mixtral exhibits lower bias.  **[CORRECT]**\\nD) The figure does not provide information about bias.\\n\\n**Figure 6: LMSys Leaderboard**\\n\\n**Question 8:** As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models? \\n\\nA) First  **[CORRECT]**\\nB) Second\\nC) Third\\nD) Fourth\\n\\n**Figure 7: Proportion of tokens assigned to each expert**\\n\\n**Question 9:** According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?\\n\\nA) Yes, experts clearly specialize in specific domains.\\nB) No, there is no clear pattern of specialization.  **[CORRECT]**\\nC) The figure does not provide information about expert specialization.\\nD) Only the first layer shows a clear pattern of specialization.\\n\\n**Table 5: Percentage of expert assignment repetitions**\\n\\n**Question 10:**  What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?\\n\\nA) Repetitions are significantly lower at higher layers.\\nB) Repetitions are similar across all layers.\\nC) Repetitions are significantly higher at higher layers.  **[CORRECT]**\\nD) The table does not provide information about expert assignment repetitions. \\n'\n",
    "display(Markdown(mixtral_response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the results are remarkably good. We observe the following:\n",
    "* Every question is correctly referring to one figure or table, even in the case where the authors inconsistently name a table as Figure 5.\n",
    "* All multiple choice options are consistent with the content of the paper.\n",
    "* All answers marked as correct are accurate.\n",
    "* The quiz exhibits comprehension of a broad range of diagrams and figures. It also clearly takes into account at least the text accompanying each figure or table (caption).\n",
    "* In one case, for question 5, the model even performs a substraction, adding a layer of reasoning beyond simply extracting/reading information.\n",
    "\n",
    "We observe the following limitations:\n",
    "* The format is different from the one chosen previously, so it'd be hard to post-process to extract question options and correct answer, among other aspects of interest.\n",
    "* The model stops at 10 questions, even though there are more figures and tables in the document. Other generations generate even fewer questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structured generation\n",
    "\n",
    "In order to be able to create a dataset by automated means, it'd be desirable to have a consistent output format that we can easily parse. In other words, we need \"structured generation\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating structured quiz...\n",
      "func_dict={'name': 'create_quiz', 'args': {'questions': [{'choices': ['1', '2', '4', '8'], 'correct_answer_idx': 1.0, 'text': 'In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?'}, {'text': 'According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?', 'correct_answer_idx': 2.0, 'choices': ['Comprehension', 'Knowledge', 'Code', 'AGI Eval']}, {'choices': ['7B', '13B', '33B', '70B'], 'correct_answer_idx': 1.0, 'text': 'According to Table 2, how many active parameters does Mixtral 8x7B use during inference?'}, {'text': 'Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?', 'correct_answer_idx': 3.0, 'choices': ['Math', 'Code', 'Reasoning', 'Comprehension']}, {'choices': ['10.9%', '8.4%', '4.9%', '2.3%'], 'correct_answer_idx': 0.0, 'text': 'On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?'}, {'text': \"What does the left part of Figure 4 demonstrate about Mixtral\\\\'s ability on the Passkey task?\", 'correct_answer_idx': 1.0, 'choices': [\"Mixtral\\\\'s perplexity decreases with longer context.\", 'Mixtral achieves perfect retrieval accuracy regardless of context length.', 'Mixtral struggles with retrieving passkeys in long sequences.', \"Mixtral\\\\'s performance is comparable to other models on this task.\"]}, {'text': \"Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\\\'s performance on bias benchmarks?\", 'correct_answer_idx': 2.0, 'choices': ['Mixtral exhibits higher bias.', 'Mixtral exhibits similar bias.', 'Mixtral exhibits lower bias.', 'The figure does not provide information about bias.']}, {'text': 'As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models?', 'correct_answer_idx': 0.0, 'choices': ['First', 'Second', 'Third', 'Fourth']}, {'choices': ['Yes, experts clearly specialize in specific domains.', 'No, there is no clear pattern of specialization.', 'The figure does not provide information about expert specialization.', 'Only the first layer shows a clear pattern of specialization.'], 'correct_answer_idx': 1.0, 'text': 'According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?'}, {'text': 'What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?', 'correct_answer_idx': 2.0, 'choices': ['Repetitions are significantly lower at higher layers.', 'Repetitions are similar across all layers.', 'Repetitions are significantly higher at higher layers.', 'The table does not provide information about expert assignment repetitions.']}]}}\n"
     ]
    }
   ],
   "source": [
    "from vertexai_utils import generate_structured_quiz\n",
    "from models import Quiz\n",
    "\n",
    "questions, metadata = generate_structured_quiz(mixtral_response)\n",
    "quiz = Quiz(article=Article(online_documents[0]), questions=questions, metadata=metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'article': {'title': 'Mixtral of Experts',\n",
       "  'uri': 'https://arxiv.org/pdf/2401.04088'},\n",
       " 'questions': [{'choices': ['1', '2', '4', '8'],\n",
       "   'correct_answer_idx': 1.0,\n",
       "   'text': 'In the Mixture of Experts Layer, how many experts are assigned to each input vector by the router?'},\n",
       "  {'text': 'According to Figure 2, in which area does Mixtral demonstrate a significantly superior performance compared to Llama 2 70B?',\n",
       "   'correct_answer_idx': 2.0,\n",
       "   'choices': ['Comprehension', 'Knowledge', 'Code', 'AGI Eval']},\n",
       "  {'choices': ['7B', '13B', '33B', '70B'],\n",
       "   'correct_answer_idx': 1.0,\n",
       "   'text': 'According to Table 2, how many active parameters does Mixtral 8x7B use during inference?'},\n",
       "  {'text': 'Based on Figure 3, in which area does Mixtral **not** consistently outperform Llama 2 70B while using fewer active parameters?',\n",
       "   'correct_answer_idx': 3.0,\n",
       "   'choices': ['Math', 'Code', 'Reasoning', 'Comprehension']},\n",
       "  {'choices': ['10.9%', '8.4%', '4.9%', '2.3%'],\n",
       "   'correct_answer_idx': 0.0,\n",
       "   'text': 'On the MBPP benchmark, what is the performance difference between Mixtral 8x7B and Llama 2 70B?'},\n",
       "  {'text': \"What does the left part of Figure 4 demonstrate about Mixtral\\\\'s ability on the Passkey task?\",\n",
       "   'correct_answer_idx': 1.0,\n",
       "   'choices': [\"Mixtral\\\\'s perplexity decreases with longer context.\",\n",
       "    'Mixtral achieves perfect retrieval accuracy regardless of context length.',\n",
       "    'Mixtral struggles with retrieving passkeys in long sequences.',\n",
       "    \"Mixtral\\\\'s performance is comparable to other models on this task.\"]},\n",
       "  {'text': \"Compared to Llama 2 70B, what does Figure 5 suggest about Mixtral\\\\'s performance on bias benchmarks?\",\n",
       "   'correct_answer_idx': 2.0,\n",
       "   'choices': ['Mixtral exhibits higher bias.',\n",
       "    'Mixtral exhibits similar bias.',\n",
       "    'Mixtral exhibits lower bias.',\n",
       "    'The figure does not provide information about bias.']},\n",
       "  {'text': 'As of December 2023, what is the ranking of Mixtral 8x7B Instruct v0.1 on the LMSys Leaderboard among open-weight models?',\n",
       "   'correct_answer_idx': 0.0,\n",
       "   'choices': ['First', 'Second', 'Third', 'Fourth']},\n",
       "  {'choices': ['Yes, experts clearly specialize in specific domains.',\n",
       "    'No, there is no clear pattern of specialization.',\n",
       "    'The figure does not provide information about expert specialization.',\n",
       "    'Only the first layer shows a clear pattern of specialization.'],\n",
       "   'correct_answer_idx': 1.0,\n",
       "   'text': 'According to Figure 7, is there a clear pattern of expert specialization based on the topic of the text across different layers?'},\n",
       "  {'text': 'What trend does Table 5 reveal about expert assignment repetitions at higher layers (15 and 31) compared to the first layer (0)?',\n",
       "   'correct_answer_idx': 2.0,\n",
       "   'choices': ['Repetitions are significantly lower at higher layers.',\n",
       "    'Repetitions are similar across all layers.',\n",
       "    'Repetitions are significantly higher at higher layers.',\n",
       "    'The table does not provide information about expert assignment repetitions.']}],\n",
       " 'metadata': {'model': 'publishers/google/models/gemini-1.5-pro-001',\n",
       "  'region': 'europe-west9',\n",
       "  'num_input_tokens': 982,\n",
       "  'num_output_tokens': 541,\n",
       "  'generation_time': 14.60603404045105,\n",
       "  'timestamp': '2024-05-29 21:36:50.085381+00:00'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe how we could convert the generated quiz into a structured format, which can be parsed into the desired output format easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
