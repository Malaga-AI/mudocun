{"article":{"title":"Mixtral of Experts","uri":"https://arxiv.org/pdf/2401.04088"},"questions":[{"multiple_choice_question":{"text":"In a Mixture of Experts Layer, how is the layer\\'s output determined?","choices":["By selecting the output of the expert with the highest gating weight.","By averaging the outputs of all expert networks.","By taking the dot product of the input vector and the gating weights.","By calculating the weighted sum of the outputs of the selected experts."],"correct_answer_idx":3},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What is the context length used in the Mixtral model architecture?","choices":["128 tokens","32,768 tokens ","4,096 tokens ","32,000 tokens "],"correct_answer_idx":1},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"Compared to Llama 2 70B, in which areas does Mixtral demonstrate significantly superior performance?","choices":["Reading comprehension and AGI Eval","Mathematics and code generation ","Knowledge-based tasks and MMLU","Commonsense reasoning and BBH"],"correct_answer_idx":1},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"While Mixtral outperforms Llama 2 70B on most benchmarks, on which type of benchmark does Llama 2 70B occasionally outperform Mixtral, despite having 5x more active parameters?","choices":["Knowledge-based tasks ","Reading comprehension","Commonsense reasoning ","Code generation"],"correct_answer_idx":1},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What does the right-hand side graph in Figure 4 demonstrate about Mixtral\\'s performance?","choices":["Mixtral achieves a consistent perplexity score regardless of context length.","Mixtral struggles to maintain accuracy on the Passkey task with longer sequences.","Mixtral\\'s perplexity decreases as the context length increases, demonstrating its ability to handle long-range dependencies. ","Mixtral\\'s performance on the proof-pile dataset is significantly worse than on the Passkey task."],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}}],"metadata":{"creation_metadata":{"model":"publishers/google/models/gemini-1.5-pro-001","region":"europe-west9","num_input_tokens":3376,"num_output_tokens":597,"generation_time":51.748371839523315,"timestamp":"2024-05-30 18:02:17.939732+00:00"},"structuring_metadata":{"model":"publishers/google/models/gemini-1.5-pro-001","region":"europe-west9","num_input_tokens":706,"num_output_tokens":341,"generation_time":10.065201759338379,"timestamp":"2024-05-30 18:02:28.008852+00:00"}}}