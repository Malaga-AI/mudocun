# Mudocun: A synthetic dataset for multimodal understanding of scientific documents

Mudocun is a synthetic dataset to check understanding of scientific documents. It currently contains  **4,800 multiple choice A-D questions extracted from 866 scientific articles**, created by Google's Gemini 1.5 pro.

All questions include answers and are stored in a structured format, for easier downstream processing.

Mudocun focuses on charts and formulas, while referring to context provided in the surrounding text, to stress the model capabilities to reason around multiple modalities provided in the document.

This dataset can be used to evaluate multimodal AI systems (e.g. multimodal LLMs, multimodal RAG systems) focusing on scientific publications across text, formulas and images.

It can be used for:
* benchmarking: evaluate how your target system compares against reference model, Gemini 1.5 pro
* fine-tuning: prepare your system for improved understanding of scientific documents


âœ¨ Highlights:
* The format of the question is of multiple choice, A-D, over a particular image, chart, or formula.
* Questions are valid JSON objects, that can be easily and consistenly parsed by clients.
* All questions are created by Gemini 1.5 pro. Credits were provided by community members.
* Every question has an answer as ground truth that can be held out for benchmarking. Note that the answer was automatically generated by Gemini 1.5 pro and may be inaccurate. Specific fields are provided for adding human labels, if needed.
* Reference documents span several fields, such as computer science, AI, astrophysics, etc.

Reference articles have been contributed by community members (at `docs.py`) or extracted from the excellent [Paperscape data](https://github.com/paperscape/paperscape-data) repository by Damien P. George and Robert Knegjens, in particular from articles from their 2017 listing [here](https://github.com/paperscape/paperscape-data/blob/master/pscp-2017.csv).

## Rationale

This dataset is based on the observation of the excellent multimodal capabilities exhibited by Gemini 1.5 pro, when tested with scientific pdf documents. Head over to our notebook [here](./notebooks/feasibility_study.ipynb) for a more detailed motivation.

## Configure

If you want to run request against Gemini, you'll need to set up your environment and GCP account first.

Note that this is only needed if you'd like to contribute new results.

You can proceed as follows (a virtual environment is recommended):
```bash
pip install -r requirements.txt
gcloud auth application-default login
gcloud auth application-default set-quota-project <PROJECT_NAME>
```
where `<PROJECT_NAME>` is the project defined in Vertex AI.

Additionally, create an environment file with the following variables:
```bash
PROJECT=<PROJECT_NAME>  # As defined in Vertex AI, e.g. mudocun
REGION=<GCP_REGION>   # According to your location, e.g. europe-west9
```

## Generating questions for the dataset

You can perform the generation requests against Gemini by running:
```bash
python main.py
```

This will generate questions for the documents available at `docs.py` in a JSON file in the `outputs` directory. Eventual failures will be logged in the `outputs/failed` directory. The script will continue despite eventual failures until exhausting the list of documents provided.

If the execution is restarted, the documents for which there is already an output in the designated outputs directory will be skipped, to avoid accidental overwrite (unless the `--refresh` command line option is provided).

Here is the full list of parameters:
* `--refresh`: generate questions for every documents, even if one already exists.
* `--arxiv-docs`: take documents from the Paperscape data 2017 listing as input, instead of the documents at `docs.py`.
* `--output-dir`: directory to store the JSON files containing the generated multiple choice questions (default is `./outputs`). The directory must exist previously.
* `--failed-dir`: directory to store JSON files with error logs (default is `./outputs/failed`). The directory must exist previously.
* `--start_index`: index from the list of input documents to start the generation (default is 0). This is useful if you need to resume the data collection at some arbitrary point in the list.
