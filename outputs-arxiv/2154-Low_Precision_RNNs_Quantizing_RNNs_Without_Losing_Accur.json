{"article":{"title":"Low Precision RNNs: Quantizing RNNs Without Losing Accuracy","uri":"https://arxiv.org/pdf/1710.07706"},"questions":[{"multiple_choice_question":{"text":"Which of the following best describes the concept illustrated in Figure 1?","choices":["The structure of a convolutional neural network.","The process of backpropagation in a neural network.","The unrolled structure of a recurrent neural network cell over time.","The difference between feedforward and feedback neural networks."],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"According to Table 1, what is the impact of reducing the weight bit-width from 32 to 8 on the perplexity per word (PPW) of the Bit-RNN model when the activation bit-width is kept at 32?","choices":["PPW increases by approximately 1%.","PPW decreases by approximately 1%.","PPW increases by approximately 8%.","PPW decreases by approximately 8%."],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What is the primary finding demonstrated by Table 2 regarding the relationship between bit-width, number of neurons, and PPW?","choices":["Increasing the number of neurons always leads to improved PPW, regardless of bit-width.","Reducing bit-width always leads to worse PPW, even with increased neurons.","A balanced reduction in bit-width with a corresponding increase in neurons can maintain or even improve PPW.","There is no clear relationship between bit-width, number of neurons, and PPW."],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"Based on the data presented in Table 3, what conclusion can be drawn about quantizing only the FC layers in the Deep Speech model?","choices":["Quantizing the FC layers to 8 bits leads to a significant drop in accuracy, regardless of neuron increase.","Quantizing the FC layers to 4 bits is not feasible, as the accuracy loss is too significant.","Increasing the number of neurons in specific FC layers can compensate for accuracy loss due to quantization, even at 4 bits.","Neuron increases have no impact on the accuracy of the model when FC layers are quantized."],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"Table 4 focuses on the impact of quantizing the recurrent layer activations in the Deep Speech model.  What key takeaway can be drawn from the data presented?","choices":["Quantizing the recurrent layer activations has a much larger negative impact on accuracy than quantizing the FC layers.","Quantizing the recurrent layer activations has a minimal impact on accuracy, even when reducing to 4 bits.","Quantizing the recurrent layer activations to 8 bits is acceptable, but 4 bits leads to unacceptable accuracy loss.","The data is inconclusive and does not provide a clear understanding of the impact of quantizing recurrent layer activations."],"correct_answer_idx":1},"metadata":{"is_validated":false,"validator":null,"explanation":null}}],"metadata":{"creation_metadata":{"model":"publishers/google/models/gemini-1.5-pro-001","region":"europe-west9","num_input_tokens":1312,"num_output_tokens":628,"generation_time":14.978468656539917,"timestamp":"2024-06-01 08:05:59.941139+00:00"},"structuring_metadata":{"model":"publishers/google/models/gemini-1.5-pro-001","region":"europe-west9","num_input_tokens":737,"num_output_tokens":509,"generation_time":11.951570272445679,"timestamp":"2024-06-01 08:06:11.893433+00:00"}}}