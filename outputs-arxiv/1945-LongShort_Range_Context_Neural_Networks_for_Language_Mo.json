{"article":{"title":"Long-Short Range Context Neural Networks for Language Modeling","uri":"https://arxiv.org/pdf/1708.06555"},"questions":[{"multiple_choice_question":{"text":"What does Figure 1 illustrate about word triggering information in language modeling?","choices":["Word triggering is only relevant at short distances (less than three words)."," \"She\" is a more predictive pronoun than \"he\" at all distances.","Word triggering correlations are consistent across different pronoun pairs.","Word triggering information can exist at large distances, even up to one thousand words."],"correct_answer_idx":3},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What type of neural network architecture is depicted in Figure 2?","choices":["Feedforward Neural Network (FFNN)","Long-Short Term Memory Network (LSTM)","Elman Recurrent Neural Network (RNN)","Convolutional Neural Network (CNN)"],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What is the main function of the gates (i, f, o) in the LSTM module shown in Figure 3?","choices":["To process the input word embedding directly.","To regulate the flow of information within the LSTM cell.","To predict the probability distribution of the next word.","To connect the hidden state to the output layer."],"correct_answer_idx":1},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"Which component in Figure 4 represents the short-range context information in the LSRC network?","choices":["Ct-1","Ht-1","Ht","Xt-1"],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What conclusion can be drawn from the temporal correlation analysis presented in Figure 5?","choices":["The context information in LSRC changes at a similar rate to RNN."," LSTM is unable to capture long-range correlations effectively.","The LSRC global state captures long-range correlations, while the local state captures short-range correlations."," RNN demonstrates superior performance in capturing long-range correlations compared to LSRC and LSTM."],"correct_answer_idx":2},"metadata":{"is_validated":false,"validator":null,"explanation":null}},{"multiple_choice_question":{"text":"What does Figure 6 demonstrate about the relationship between hidden layer size and perplexity in different language models?","choices":["Increasing the hidden layer size consistently improves performance for all models."," LSRC achieves comparable or better performance than larger LSTM and RNN models even with smaller hidden layer sizes."," FFNN consistently outperforms recurrent models across all hidden layer sizes.","The performance of all models plateaus after a certain hidden layer size, indicating diminishing returns."],"correct_answer_idx":1},"metadata":{"is_validated":false,"validator":null,"explanation":null}}],"metadata":{"creation_metadata":{"model":"publishers/google/models/gemini-1.5-pro-001","region":"europe-west9","num_input_tokens":2344,"num_output_tokens":670,"generation_time":15.49598479270935,"timestamp":"2024-06-01 07:07:37.348016+00:00"},"structuring_metadata":{"model":"publishers/google/models/gemini-1.5-pro-001","region":"europe-west9","num_input_tokens":779,"num_output_tokens":417,"generation_time":11.840664148330688,"timestamp":"2024-06-01 07:07:49.189334+00:00"}}}